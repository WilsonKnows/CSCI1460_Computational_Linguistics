# -*- coding: utf-8 -*-
"""1460 Project5 Dependency Parsing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h3dsiPete2aUfPbZvx6ucQZx9e2IYbx-

# ðŸ”® âž¡ï¸ Assignment 5: Shift-Reduce Dependency Parser
In this assignment, you'll be implementing a dependency parser using the Shift-Reduce algorithm described in lecture on grammars and parsing. In Part 1, you'll implement the basic algorithm, with a dummy "oracle" that just uses the ground-truth labels. Then, you'll train a classifier to act as an actual oracle in Part 2, and evaluate the results of that model in Part 3. Have fun shifting!

# [DO NOT EDIT] Setup
"""

!pip install torch tqdm numpy datasets

import numpy as np
import random, torch, tqdm

from datasets import load_metric
from google.colab import output
from numpy.typing import NDArray
from torch.optim import Optimizer as TorchOptimizer
from torch.utils.data import DataLoader as TorchDataLoader
from typing import Any, Dict, List, Optional, Tuple

output.enable_custom_widget_manager()
random.seed(1460)
np.random.seed(1460)
torch.manual_seed(1460)

"""# Part 1: Basic Algorithm with Ground-Truth Oracle

Read through the following sections to understand the setup here, and then complete the TODO sections. We will define several helper functions for you.

## TODO: Download the data

We will be using two files for this assignment: `train.conll` and `test.conll`. You can download [`train.conll` here](https://drive.google.com/file/d/1Yky_NVpL0wKFpA8604TCRbT4CSKidCqK/view?usp=drive_link) and [`test.conll` here](https://drive.google.com/file/d/1J7W_cof5fCroGbC-pLIxUTCvgIDy8ufL/view?usp=drive_link). Drag them into the 'Files' sidebar in this Colab to use them in your code.

## Loading Data
The datasets for this assignment come in a particularly strange format, called CoNLL. We've handled parsing it, but we encourage you to check out the format so that you'll understand it if you ever need to work with it. All that you need to know is that `read_conll` returns a list of dictionaries, where each dictionary encodes a parsed sentence. Each dictionary has a `'words'` key, which maps to a list of words which make up the sentence, and `'heads'` key, which maps to a list of integers, where the value at index `i` is the index of the word that is word `i`'s head. The root word has a head value of `None`.  

So the sentence '1460 is awesome.' would have the words `['1460', 'is', 'awesome', '.']` (yes, the punctuation is included), with heads `[2, 2, None, 2]`. Check out the visual below which illustrates the dependency parse (arrows going out of one word and into another mean that that first word is the head of the other):

![Screenshot 2022-11-21 164002.png](data:image/png;base64,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)

As you can see, 'awesome' is the root word, and its head is hence the value `None`. 'awesome' is also the head of all of the other words, you can see this since the `'heads'` values corresponding to the other words are all `2`, which is the index of the word 'awesome' in the word list!
"""

def read_conll(in_file: str, lowercase: bool = False,
               max_example: Optional[int] = None) -> List[Dict[str, Any]]:
  """
  Function that reads in a CoNLL file and produces example parses.

  Parameters
  ----------
  in_file : str
      file path to a .conll file
  lowercase : bool
      determines whether to lowercase words in the "words" string
  max_example : Optional[int]
      the maximum number of examples to process, or None to process all

  Returns
  -------
  List[Dict[str, Any]]
      a list of examples taken from the .conll file
  """

  examples = []
  with open(in_file) as f:
    word, pos, head, label = [], [], [], []
    for line in f.readlines():
      sp = line.strip().split('\t')
      if len(sp) == 10:
        if '-' not in sp[0]:
          word.append(sp[1].lower() if lowercase else sp[1])
          pos.append(sp[4])
          head.append(int(sp[6]) - 1)
          label.append(sp[7])
      elif len(word) > 0:
        examples.append({'words': word, 'pos': pos, 'heads': head, 'label': label})
        word, pos, head, label = [], [], [], []
        if (max_example is not None) and (len(examples) == max_example):
          break
    if len(word) > 0:
      head[head.index(-1)] = None
      examples.append({'words': word, 'pos': pos, 'heads': head, 'label': label})
  return examples

"""## (DO NOT EDIT) Ground-Truth Dummy Oracle
This function has been written for you, please do not edit it! It takes the current stack, buffer, root word, as well as the full parse (read from the CoNLL), and returns the ground-truth correct action (arc left, arc right, or shift) to be taken by the Shift-reduce algorithm.
"""

def get_ground_truth_oracle(stack: List[int], buf: List[int],
                            ex: Dict[str, Any]) -> Optional[int]:
  """
  Function that retrieves the best possible action (arc left, arc right, or shift)
  for a given stack, buffer, and ground truth parsing.

  Parameters
  ----------
  stack : List[int]
      the shift-reduce stack
  buf : List[int]
      the shift-reduce buffer
  ex : Dict[str, Any]
      Raw training example dict produced by read_conll()

  Returns
  -------
  Optional[int]: {None, 0, 1, 2}
      action to apply in shift-reduce algorithm
      None -> no actions can be taken
      0 -> arc left
      1 -> arc right
      2 -> shift
  """

  if len(stack) == 1:
      return None if len(buf) == 0 else 2
  elif len(stack) == 2:
      return 1 if len(buf) == 0 else 2

  sf = stack[-1]
  ss = stack[-2]
  hf = ex['heads'][sf]
  hs = ex['heads'][ss]

  if hs == sf:
    return 0
  elif hf == ss and (not any([True for x in buf if ex['heads'][x] == sf])):
    return 1
  else:
    return None if len(buf) == 0 else 2

"""**VERY IMPORTANT:**


We need to train models using supervised learning to drive our shift-reduce algorithm. To do this, we'll break the parsing of an entire sentence down into steps, called _partial parses_. At each step a classifier will have to decide, given some features, whether to arc left, arc right, or shift. Each partial parse can be labeled with the correct action, and we can train the model to execute partial parses as well as possible!

The input to our model is a set of features, from which the correct partial parse is to be predicted. Our model will use learned word embeddings, and will classify between three classes. The model will have an embedding layer, which can be used to fetch learned embeddings for words, based on their ID. As such, the model takes some number of integers, each less than the length of the vocabulary, as input.

## word_to_id
Since we need to build up a vocabulary when training, and assign an ID/index to each word, we need some functions to handle doing so. We'll first define word_to_id, which when in `is_test=False` mode, both builds a vocabulary as well as returns the IDs of known words. When `is_test=True`, unknown words are not used to expand the vocabulary.
"""

def word_to_id(word: str, vocab: Dict[str, int], is_test: bool = False) -> int:
  """
  Function that converts a string `word` to its corresponding ID in `vocab`.
  If not in testing mode, also adds `word` to vocab.

  Parameters
  ----------
  word : str
      the word in question
  vocab : Dict[str, int]
      vocabulary mapping words to their ID
  is_test : bool
      whether to operate in testing or training mode

  Returns
  -------
  int
      the ID of `word`
  """

  if is_test:
    if word in vocab:
      return vocab[word]
    else:
      return vocab["<UNK>"]

  # otherwise, we're doing train:

  if word in vocab:
    if random.randint(0, 100) <= 5:
      return vocab['<UNK>'] # randomly mask this word for training example, otherwise <UNK> never trained on.
    return vocab[word]

  else:
    n = len(vocab)
    vocab[word] = n
    return n

"""
## TODO: `apply_action`
This is a helper function that applies a given action (arc left, arc right, or shift) to the current stack, buffer, and arcs. It should modify these and return the new stack, buffer, and arcs"""

def apply_action(stack: List[int], buf: List[int], arcs: List[Tuple[int, int]],
                 action: int) -> Tuple[List[int], List[int], List[Tuple[int, int]]]:
  """
  Function that applies `action` to the current state. Modifies stack, buf, and
  arcs accordingly and returns the new state. stack, buf, and arcs contain the
  indices of words in an example sentence, where an index of None represents the
  root.

  Parameters
  ----------
  stack : List[int]
      the current shift-reduce stack. stack[-1] is the top of stack, stack[-2] is
      the second item
  buf : List[int]
      the current shift-reduce buffer. buf[0] is the first item in the buffer
  arcs : List[Tuple[int, int]]
      the current arcs produced by shift-reduce (of the form (head idx, word idx))
  action : int
      the action we want to apply
      0 -> arc left
      1 -> arc right
      2 -> shift

  Returns
  -------
  Tuple[List[int], List[int], List[Tuple[int, int]]]
      the updated stack, buffer, and arcs
  """

  # TODO
  if action == 0:  # Arc left
      # Get the indices of the two items on top of the stack
      parent = stack[-1]
      child = stack[-2]

      # Create an arc (head idx, word idx) from parent to child
      arcs.append((parent, child))
      # Remove the child from the stack
      stack.pop(-2)

  elif action == 1:  # Arc right
      # Get the indices of the two items on top of the stack
      parent = stack[-2]
      child = stack[-1]

      # Create an arc (head idx, word idx) from parent to child
      arcs.append((parent, child))
      # Remove the child from the stack
      stack.pop()

  elif action == 2:  # Shift
      # Move the first item from the buffer to the top of the stack
      stack.append(buf[0])
      # Remove the first item from the buffer
      buf.pop(0)

  return stack, buf, arcs

"""## TODO: `is_possible`
`is_possible` returns whether an action (arc or shift) is possible given the current stack and buffer. This will be used in part 2 once we start training an oracle using machine learning.
"""

def is_possible(action: int, stack: List[int], buf: List[int]) -> bool:
  """
  Function that determines if a given action is possible given the current stack
  and buffer state. Remember that stack and buf contain the indices of words in
  an example sentence, where an index of None represents the root. Think carefully
  about this function; it is likely to be a source of errors. Work through the
  example in the slides and try to replicate that logic here. Be particularly
  careful with the root (None). Can anything be the head of root? Should we ever
  perform an arc with the root while there are still words in the buffer?
  Remember that at the beginning of the shift-reduce algorithm, stack and buf
  will look like:
      stack = [None]
      buf = [0, 1, 2, ...]

  Parameters
  ----------
  action : int
      the action we want to know is possible
      0 -> arc left
      1 -> arc right
      2 -> shift
  stack : List[int]
      the shift-reduce stack
  buf : List[int]
      the shift-reduce buffer

  Returns
  -------
  bool
      whether or not the given action is possible
  """

  # TODO
  if action == 0: # Arc left
      return len(stack) > 2

  elif action == 1: # Arc right
    if buf:
      return len(stack) > 2
    else:
      return len(stack) >= 2

  elif action == 2:
    return len(buf) > 0

"""## TODO: `get_features`
This is a featurization function - it takes in the stack, buffer, words to be parsed, and vocabulary (as well as an `is_test` flag) and returns a featurization to be passed to the classifier. This featurization should consist of three word IDs to be converted to embeddings.

You may decide how to featurize the stack and buffer such that the classifier executes partial parses with the best results possible, **however our autograder will expect a feature vector of length three**. `stack` and `buf` are both lists of integers that represent indices of strings in the list `words` (with `None` representing the root node). Use these indices to get the corresponding word strings, and then use `word_to_id` to convert words to their IDs. **Note**: for the root use the `'<ROOT>'` token. Remember that our features are in terms of **word ids** in our `vocab`, not **word indices** that the `stack` and `buf` contain!

Keep in mind that you must always output the same number of features. If one of your features is the first word in the buffer, but the buffer is empty, you can provide the ID corresponding to the `'<NW>'` ('no word') token to fill its place!
"""

def get_features(stack: List[int], buf: List[int], words: List[str],
                     vocab: Dict[str, int], is_test: bool = False) -> List[int]:
  """
  Function that takes in the current state (stack, buffer) and produces a set of
  features to be fed into a classifier. Think about what information our
  classifier would need to know in order to predict an optimal action from the
  current state. Hint: you should only need 3 features

  Parameters
  ----------
  stack : List[int]
      the shift-reduce stack
  buf : List[int]
      the shift-reduce buffer
  words : List[str]
      words in the sentence, in order
  vocab : Dict[str, int]
      the word id vocabulary
  is_test : bool
      whether we are in testing mode

  Returns
  -------
  List[int]
      A list of three integers to be fed into the classifier
      Hint: use word_to_id on the items of the data structures
  """

  features = []

  #TODO

  # Feature 1: Word ID from the top of the stack
  if stack:
      top_word = words[stack[-1]] if stack[-1] is not None else '<ROOT>'
      features.append(word_to_id(top_word, vocab, is_test))
  else:
      features.append(word_to_id('<NW>', vocab, is_test))  # No word on the stack, use a placeholder

  # Feature 2: Word ID from the first item in the buffer
  if buf:
      next_word = words[buf[0]]
      features.append(word_to_id(next_word, vocab, is_test))
  else:
      features.append(word_to_id('<NW>', vocab, is_test))  # No word in the buffer, use a placeholder

  # Feature 3: Word ID from the second item in the stack (if available)
  if len(stack) > 1:
      top_word2 = words[stack[-2]] if stack[-2] is not None else '<ROOT>'
      features.append(word_to_id(top_word2, vocab, is_test))
  else:
      # If there is no second item in the stack, use the '<NW>' token
      features.append(word_to_id("<NW>", vocab, is_test)) # No word on the stack -2, use a placeholder

  return features

# feel free to test your bow_features here! Modify this example as much as you want
# original: "I prefer the morning flight through Denver ."
example = {'words': "I prefer the morning flight through Denver .".split(" "), 'heads': [1, None, 4, 4, 1, 6, 1, 1]}
fake_vocab = {'<ROOT>': 0, '<UNK>': 1, '<NW>': 2, "I": 3, "prefer": 4, "the": 5, "morning": 6, "flight": 7, "through": 8, "Denver": 9, ".": 10}
# simulate some part of the process, edit stack and buf as you want
stack = [None]                   # stack initialized with root index
buf = [0, 1, 2, 3, 4, 5, 6, 7] # buf initialized with word indices
is_test=False
features = get_features(stack, buf, example['words'], fake_vocab, is_test)
assert len(features) == 3
print(features)

"""## TODO: `oracle_shift_reduce`
This is one of the two implementations of shift reduce in this assignment. It applies the regular shift-reduce algorithm to generate partial parses for training, as well as the correct arcs for a labeled example.

Apply the shift-reduce algorithm, making use of the functions `get_ground_truth_oracle`, `get_features`, and `apply_action` (and possibly more!), in order to both produce a full parse of the input example as well as labeled partial parses. In other words, for each partial parse along the way, we want you to save that state in `featurized_states` with the featurization from your `get_features` function, as well as the "gold" action produced by the ground-truth oracle.

Remember to handle the case where `get_ground_truth_oracle` returns `None`.
"""

def oracle_shift_reduce(example: Dict[str, Any], vocab: Dict[str, int],
                        is_test: bool = False) -> Tuple[List[List[int]], List[int], List[Tuple[int, int]]]:
  """
  Function that executes the ideal shift reduce algorithm determined by the oracle
  on a single example and returns a set of features and labels to pass into the classifier.
  This function will be called for every example parse to generate our training
  data.

  Parameters
  ----------
  example : Dict[str, Any]
      a training example produced by read_conll
  vocab : Dict[str, int]
      the vocabulary
  is_test : bool
      whether the function is being used to generate training or testing data.

  Returns
  -------
  Tuple[List[List[int]], List[int], List[Tuple[int, int]]]
      featurized states, ground truth actions, and final set of arcs.
      We're not going to need the arcs when generating the data, but they'll
      be useful later on when we assess our model's performance!
  """

  featurized_states = []
  ground_truth_actions = []

  # TODO
  # Initialize stack, buffer, and arcs
  stack = [None]  # The stack starts with only the root (None)
  buf = list(range(len(example['words'])))  # Initialize buffer with word indices
  arcs = []  # List to store arcs (head, dependent)

  while buf or len(stack) > 1:
    # Get the action based on the ground truth oracle
    action = get_ground_truth_oracle(stack, buf, example)

    # Check if the action is possible
    if is_possible(action, stack, buf):
        # Get features for the current state
        features = get_features(stack, buf, example['words'], vocab, is_test)

        # Append the featurized state and ground truth action
        featurized_states.append(features)
        ground_truth_actions.append(action)

        # Apply the action to update the stack, buffer, and arcs
        stack, buf, arcs = apply_action(stack, buf, arcs, action)
    else:
        # If the action is not possible, exit the loop
        break

  return featurized_states, ground_truth_actions, arcs

"""# Part 2: Machine Learned Oracle
Above, you implemented a version of shift-reduce that assumed you had a ground-truth oracle. Of course, in practice, you will never have this! So, in order to run your parser on unseen data, you will need to implement another version of the oracle--this time using machine learning!

## `generate_training_data`
First, we need to get training data to train our machine-learned oracle. To do this, we can use our ground truth oracle from above to generate training/testing instances (labeled partial parses) for us to train our classifier on. This essentially entails applying `oracle_shift_reduce` to every example in the raw data.
"""

def generate_training_data(raw_data: List[Dict[str, Any]], vocab: Dict[str, int],
                           is_test: bool = False) -> Tuple[List[List[int]], List[int]]:
  """
  Function that generates training data and labels from a list of examples
  produced by read_conll().

  Parameters
  ----------
  raw_data : List[Dict[str, Any]]
      a list of example parses loaded with read_conll
  vocab : Dict[str, int]
      the vocabulary to use
  is_test : bool
      whether to create testing or training data

  Returns
  -------
  Tuple[List[List[int]], List[int]]
      a set of featurized states and corresponding ground truth actions
  """

  X, Y = [], []

  # TODO
  for example in raw_data:
      featurized_states, ground_truth_actions, _ = oracle_shift_reduce(example, vocab, is_test)
      X.extend(featurized_states)
      Y.extend(ground_truth_actions)

  return X, Y

"""### Dataset objects
Since this assignment uses Pytorch modules as the classifiers for the shift-reduce algorithm, we'll make a dataset child class for our data, such that we can eventually make PyTorch Dataloaders (just as in assignment 4)! This has been implemented for you.

"""

# def of Dataset child obj to use with DataLoader
class SRDataset(torch.utils.data.Dataset):
  def __init__(self, X: List[List[int]], Y: List[int]) -> None:
    """
    Function that initializes fields for the dataset. Anything you need in
    `__init__()` or `__getitem__()` should be saved here by defining any
    number of self.*

    Parameters
    ----------
    X : List[List[int]]
        our featurized data from `generate_training_data()`
    Y : List[int]
        one-hot encoded labels corresponding to each entry in X

    Returns
    -------
    None
    """

    self.X = np.zeros((len(X), len(X[0])), dtype='int32')
    self.Y = np.zeros((len(Y), 3), dtype='float32')
    for i, class_num in enumerate(Y):
      self.X[i] = X[i]
      self.Y[i, class_num] = 1

  def __len__(self) -> int:
    """
    Function that returns the number of training examples.

    Parameters
    ----------
    self : refers to the current object

    Returns
    -------
    int
        the number of entries in X
    """
    return len(self.Y)

  def __getitem__(self, idx: int) -> Dict[str, NDArray[Any]]:
    """
    Function that defines how to retrieve an entry and a label from the data given
    and index.

    Parameters
    ----------
    idx : int
        the index of an example in X or Y
    Returns
    -------
    Dict[str, NDArray[Any]]
        a dictionary defining the example and corresponding label described by
        index `idx`
    """
    return {'inputs': self.X[idx], 'targets': self.Y[idx]}

"""### Training Function

Here we define our function for training the model. This has been implemented for you
"""

def train(model: torch.nn.Module, dataloader: TorchDataLoader, epochs: int,
          optimizer: TorchOptimizer, metric_names: List[str]) -> None:
  """
  Function that trains our classifier.

  Parameters
  ----------
  model : torch.nn.Module
      the classifier we want to train
  dataloader : torch.utils.data.DataLoader
      the dataloader containing our training data
  epochs : int
      number of epochs to train the model
  optimizer : torch.optim.Optimizer
      the optimizer to user for training
  metric_names : List[str]
      list of metric names to compute

  Returns
  -------
  None
  """
  model.train()
  loss_func = torch.nn.CrossEntropyLoss()

  for epoch in range(epochs):
    print(f"epoch {epoch}")
    progress_bar = tqdm.notebook.tqdm(range(len(dataloader)))

    metrics = [load_metric(x) for x in metric_names]

    for batch in dataloader:
      batch = {k: v.to(device) for k, v in batch.items()}

      outputs = model(**batch)

      loss = loss_func(outputs['output'], outputs['labels'])
      loss.backward()

      predictions = torch.argmax(outputs['output'], dim=1)
      references = torch.argmax(outputs['labels'], dim=-1)

      for metric in metrics:
        metric.add_batch(predictions=predictions, references=references)

      optimizer.step()
      optimizer.zero_grad()
      progress_bar.update(1)

    for metric in metrics:
      print(metric.compute())

"""## TODO: `SRClassifier`
Now, let's design a simple neural classifier to train on the partial parse classification task.

It needs to have an embedding layer of size `vocab_size`, with embedding dimension of your choosing. After converting your word ids to embeddings, make sure you concatenate them together into one long vector so that they play nicely with your linear layers. We've provided a `num_features` argument such that you can make your classifier work with any number of input word IDs (which will make your life easier if you use it). Then, it's going to need several linear layers and activation functions (two hidden layers should be enough), eventually ending with three output classes (corresponding to our three possible actions). In `forward`, we need to specify how the model will be applied to input features.

PyTorch documentation will be very helpful here! You should look at the `torch.nn.Embedding` layer for word embeddings, you may find the examples helpful in figuring out what to do with its output.
"""

# model that takes input in format given by SRDataset
class SRClassifier(torch.nn.Module):
  def __init__(self, vocab_size: int, num_features: int) -> None:
    """
    Function that initializes fields for this model. This is where you can
    your model's architecture.

    Parameters
    ----------
    vocab_size : int
        the number of words in our vocabulary
    num_features : int
        the number of features for each training example (same as length of the
        list returned by `get_features()`)

    Returns
    -------
    None
    """
    super(SRClassifier, self).__init__()

    # TODO
    # Create an embedding layer to convert word IDs to embeddings
    self.embedding = torch.nn.Embedding(vocab_size, embedding_dim=100)

    # Define hidden layers
    hidden_size = 128
    self.hidden1 = torch.nn.Linear(num_features * 100, hidden_size)
    self.hidden2 = torch.nn.Linear(hidden_size, hidden_size)

    # Define the output layer
    self.output = torch.nn.Linear(hidden_size, 3)  # Output classes: 0, 1, 2


  def forward(self, inputs: torch.IntTensor,
              targets: Optional[torch.IntTensor] = None) -> Dict[str, torch.FloatTensor | torch.IntTensor]: # notice that the arg names need to match SRDataset.__getitem__() keys
    """
    Function that defines the forward pass for this model.

    Parameters
    ----------
    inputs : torch.IntTensor
        a tensor passed into our model. When training/testing, the torch
        dataloader automatically casts our numpy arrays to tensors.
    targets : Optional[torch.IntTensor]
        a tensor representing labels for each training example in `inputs`. If
        not training, this parameter can be None.

    Returns
    -------
    Dict[str, torch.FloatTensor | torch.IntTensor]
        a dictionary containing our model's output and corresponding labels.
    """

    #TODO - Pass inputs (numpy array of word indices) through your classifier
    # Pass inputs (word IDs) through the embedding layer
    embeddings = self.embedding(inputs)

    # # Concatenate the embeddings into one long vector
    embeddings = embeddings.view(-1, 300) # embeddings.view(embeddings.size(0), -1)

    # print(embeddings.shape)

    # Apply hidden layers with ReLU activation
    hidden_output1 = torch.relu(self.hidden1(embeddings))
    hidden_output2 = torch.relu(self.hidden2(hidden_output1))

    # Apply the output layer
    outputs = self.output(hidden_output2)

    return {'output': outputs, 'labels': targets}

"""## TODO: Training the classifier
Let's initialize some required data that will be used in training. This includes our vocabulary, which will be built when loading training data. By default, the vocabulary should map `'<UNK>'`, `'<NW>'`, and `'<ROOT>'` to start.
"""

vocabulary = {'<ROOT>': 0, '<UNK>': 1, '<NW>': 2}

"""We load the training data using `read_conll`, generate training data using `generate_training_data` and instantiate a `SRDataset` object with them. Finally, create a dataloader with the `SRDataset` object - the batch size can be very high, but if you somehow run out of memory, you should decrease it. *Nothing TODO here!*"""

raw_training_data = read_conll('train.conll')

X_train, Y_train = generate_training_data(raw_training_data, vocabulary)

train_dataset = SRDataset(X_train, Y_train)
train_dataloader = TorchDataLoader(train_dataset, batch_size=1024)

# At this point, each partial parse should be a n-tuple of integers.
print(X_train[:10])
print(Y_train[:10])

"""Next, let's instantiate our classifier and set up training hyperparameters and the optimizer."""

# setting up the classifier
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SRClassifier(len(vocabulary), 3) # instantiate classifier
print(model)

# hyperparameters:
# TODO: Choose hyperparameters
# Note: Maybe start with ~4-5 epochs, it depends on your featurization - you'll likely have to play around with it
epochs = 4  # You can adjust this based on the model's performance

# optimizer, device, and such
# AdamW works well... Try an initial learning rate of 5e-3. Again, you may find that tuning this gives
# you better performance!
# optimizer = ...
learning_rate = 5e-3  # Initial learning rate
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # You can tune learning rate here
model.to(device)

metrics = ['accuracy']

train(model, train_dataloader, epochs, optimizer, metrics)

"""## Evaluating the classifier
We can evaluate the classifer to get an idea of its accuracy before we use it as an oracle in the shift-reduce algorithm. First, load the testing data and generate the testing instances, afterwhich you should instantiate `SRDataset` and `DataLoader` objects similarly to when training. REMEMBER: Now that we're testing, we don't want to augment the vocabulary, thus we pass `is_test=True` to `generate_training_data`.
"""

test_data = read_conll('test.conll')

X_test, Y_test = generate_training_data(test_data, vocabulary, is_test=True) # is_test is true here!

test_dataset = SRDataset(X_test, Y_test)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256)

def evaluate(model: torch.nn.Module, dataloader: TorchDataLoader,
             metric_names: List[str]) -> None:
  """
  Function that evaluates a trained model.

  Parameters
  ----------
  model : torch.nn.Module
      the 1st param name `first`
  dataloader : torch.utils.data.DataLoader
      the dataloader containing our testing data
  metric_names : List[str]
      a list of metric names to compute

  Returns
  -------
  None
  """
  model.eval()
  metrics = [load_metric(metric) for metric in metric_names]

  print("evaluation: ")
  progress_bar = tqdm.notebook.tqdm(range(len(dataloader)))

  for batch in dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
      outputs = model(**batch)

    predictions = torch.argmax(outputs['output'], dim=1)
    references = torch.argmax(outputs['labels'], dim=-1)

    for metric in metrics:
      metric.add_batch(predictions=predictions, references=references)

    progress_bar.update(1)

  for metric in metrics:
    print(metric.compute())

"""We can evaluate the model to assess its performance at correctly executing partial parses!"""

metrics = ['accuracy']
evaluate(model, test_dataloader, metrics)

"""# Part 3: Evaluation and Visualization

Ok, so we've evaluated the classifier, and it seems to perform well. However, the accuracy score it recieved above doesn't quite represent the performance of our algorithm overall when it is actually applied. To measure this, we use something called UAS - **Unlabeled Attachment Score**. UAS measures the proportion of correctly assigned heads in a full parse. So far, we've only really been considering partial parses. Since a full parse requires sequentially executing partial parses, if one partial parse is wrong, it could influence future partial parses and lead to further incorrect partial parses. Thus, we'd expect our UAS to be *worse* than the accuracy we previously saw, which is more reflective of how well the algorithm is actually doing.

## TODO: `get_action`

This function takes in the classifier's prediction (in the form of a tensor) and outputs the corresponding action. Make sure to only return an action that is possible given the current stack and buffer state.
"""

def get_action(pred: torch.DoubleTensor, stack: List[int],
               buf: List[int]) -> Optional[int]:
  """
  Function that takes a prediction (tensor) and finds the highest probability
  action that is possible.

  Parameters
  ----------
  pred : torch.DoubleTensor
      our classifier's prediction, the "output" field of a torch.nn.Module (such
      as our model)
  stack : List[int]
      the shift-reduce stack
  buf : List[int]
      the shift-reduce buffer

  Returns
  -------
  Optional[int]: {None, 0, 1, 2}
      the predicted action.
      None -> no actions can be taken
      0 -> arc left
      1 -> arc right
      2 -> shift
  """

  # TODO
  # Sort the predicted actions in descending order
  sorted_indices = torch.argsort(pred, dim=1, descending=True)

  for indice_tensor in sorted_indices[0]:

    indice = indice_tensor.item()

    if (is_possible(indice, stack, buf)):

      return indice

  return None

"""## TODO: `model_shift_reduce`
Here, we define a version of shift-reduce that consults an SRClassifier, rather than the oracle from Part 1, in order to determine which actions to apply. Use as many helpers from above as needed to write this.
"""

def model_shift_reduce(model: torch.nn.Module, example: Dict[str, Any],
                       vocab: Dict[str, int]) -> List[Tuple[int, int]]:
  """
  Function that uses our trained classifier to execute the shift-reduce algorithm
  and returns the final list of arcs. Note that you will have to cast the output
  of `get_features()` to a tensor and load it onto the GPU. You can do this using
  `torch.tensor(...).to(device)` or `torch.tensor(..., device=device)`. Note that
  `device` was defined a few cells up, when we instantiated our model.

  Parameters
  ----------
  model : torch.nn.Module
      our trained classifier to use for shift-reduce
  example : Dict[str, Any]
      a training example produced by read_conll
  vocab : Dict[str, int]
      our vocabulary

  Returns
  -------
  List[Tuple[int, int]]
      the arcs of the dependency parse tree
  """

  # init stack, buf, arcs
  stack = [None]  # The stack starts with only the root (None)
  buf = list(range(len(example['words'])))
  arcs = []  # List to store arcs (head, dependent)

  # TODO
  while buf or len(stack) > 1:
    # Get features for the current state
    features = get_features(stack, buf, example['words'], vocab, is_test=True)

    # Convert features to a tensor and load it onto the GPU
    features_tensor = torch.tensor(features, device=device)

    # Get the model's prediction
    result = model(features_tensor)
    pred = result['output']

    # Get the predicted action
    action = get_action(pred, stack, buf)

    if action is None:
        # No valid action is possible, exit the loop
        break

    # Apply the action to update the stack, buffer, and arcs
    stack, buf, arcs = apply_action(stack, buf, arcs, action)

  return arcs

"""## TODO: `evaluate_UAS`
Now that we can directly compare the true parse to the model's parse, we can tackle evaluating the UAS. To evaluate UAS, we first compute both the correct and predicted parses for each testing phrase and then find the number of arcs in common divided by the number of total arcs. Then, we average these scores across all testing phrases and output the result as UAS of the model on the testing set.

$$n: \text{the total number of example parses}$$
$$M_i: \text{the set of our model's predicted arcs for example }i$$
$$O_i: \text{the set of the oracle's predicted arcs for example }i$$
$$\text{UAS} = \frac{1}{n} \sum_{i=1}^n \frac{|M_i \cap O_i|}{|O_i|}$$
"""

def evaluate_UAS(model: torch.nn.Module, test_examples: List[Dict[str, Any]],
                 vocab: Dict[str, int]) -> float:
  """
  Function that calculates UAS.

  Parameters
  ----------
  model : torch.nn.Module
      our trained classifier
  test_examples : List[Dict[str, Any]]
      test examples loaded with read_conll
  vocab : Dict[str, int]
      our word2id vocabulary

  Returns
  -------
  float
      UAS of the classifier on test_examples
  """

  # TODO
  total_correct_arcs = 0  # Total arcs
  # correct_arcs = 0  # Correctly predicted arcs

  for example in test_examples:

      total_arcs = 0
      correct_arcs = 0

      # Get the oracle's predicted arcs for the example
      _, _, oracle_arcs = oracle_shift_reduce(example, vocab, is_test=True)
      # Get the model's predicted arcs for the example
      predicted_arcs = model_shift_reduce(model, example, vocab)

      total_arcs += len(oracle_arcs)

      # Calculate the number of correct arcs
      for arc in predicted_arcs:
          if arc in oracle_arcs:
              correct_arcs += 1
      total_correct_arcs += correct_arcs / total_arcs

  # Calculate the UAS
  if len(test_examples) > 0:
      uas = total_correct_arcs / len(test_examples) # total_arcs
  else:
      uas = 0.0

  return uas

"""### Baseline UAS
Of course, we'd have no idea if our model was useful if we didn't evaluate a baseline model too. The baseline model should output something of the same format as the `SRClassifier`, except it just outputs a tensor with randomly ordered values.
"""

# model that takes input in format given by SRDataset
class SRBaseline(torch.nn.Module):
    def __init__(self) -> None:
        super(SRBaseline, self).__init__()

    def forward(self, inputs: Any) -> Dict[str, torch.Tensor]:
        return {'output': torch.randperm(3)[None, :]}

"""## TODO: Final Scoring
Evaluate the baseline UAS and compare it to that of our model. For this function, directly use the CoNLL dataset as the argument to `test_examples`, not the dataloader we had used for training.
"""

# TODO: Fill in below!
model_uas = evaluate_UAS(model, test_data, vocabulary) # evaluate_UAS(...)
baseline_model = SRBaseline()
baseline_uas = evaluate_UAS(baseline_model, test_data, vocabulary) # evaluate_UAS(...)
print(f"Model UAS: {model_uas}, baseline: {baseline_uas}")

"""The model should be over three times better than the baseline. For full credit, your model should have a UAS of over 65%, so you may need to make adjustments to your model, featurization, and hyperparameters in order to improve your score. If you submit with a UAS of under 65%, you'll recieve partial credit.

# Part 4: Conceptual
Let's explore some of the parses visually. The following functions deal with reformating and displaying dependency parses. You'll only need to use `display_parse`, check its signature. Additionally, you'll be asked to compare the parses that your model creates with real parses.

## Helpers and examples
"""

import spacy
from spacy import displacy
from spacy.displacy.render import DependencyRenderer
from IPython.core.display import display, HTML

def reformat_dep(arcs: List[Tuple[int, int]], words: List[str]) -> Tuple[List[str], List[int]]:
  """
  Function that takes in a set of arcs and produces a list of heads,
  in the same format we described at the beginning.

  Parameters
  ----------
  arcs : List[Tuple[int, int]]
      a list of arcs returned by model_shift_reduce
  words : List[str]
      a list of words from a testing example

  Returns
  -------
  Tuple[List[str], List[int]]
      the words parameters and the list of heads
  """
  heads = list(range(len(words)))
  for arc in arcs:
    heads[arc[1]] = arc[0]
  return words, heads

def parse_for_displacy(doc: List[str], heads: List[int]) -> Dict[str, Any]:
  """
  Function that takes in a sentence and a list of heads and reformats them into
  something that can be rendered using displacy.

  Parameters
  ----------
  doc : List[str]
      a list of words in a sentence
  heads : List[int]
      a list of indices indexing `doc` that represent the head of each word

  Returns
  -------
  Dict[str, Any]
      the parsed sentence in a format friendly to displacy
  """
  #doc = ['word1', 'word2', ...]
  #heads = [idx_of_head_of_word1, idx_of_head_of_word2, ...]
  settings = {
      "lang": 'en',
      "direction": 'ltr',
  }
  words = [
      {
          "text": w,#w.text,
          "tag": '',#w.tag_ if fine_grained else w.pos_,
          "lemma": w#w.lemma_ if add_lemma else None,
      }
      for w in doc
  ]
  arcs = []
  for i,word in enumerate(doc):
      if heads[i] == None:
        continue
      if i < heads[i]:
          arcs.append(
              {"start": i, "end": heads[i], "label": '', "dir": "left"}
          )
      elif i > heads[i]:
          arcs.append(
              {
                  "start": heads[i],
                  "end": i,
                  "label": '',
                  "dir": "right",
              }
          )
  return {"words": words, "arcs": arcs, "settings": settings}

def display_parse(arcs: List[Tuple[int, int]], words: List[str]) -> None:
  """
  Function that renders and displays a parsed sentence.

  Parameters
  ----------
  arcs : List[Tuple[int, int]]
      the final set of arcs returned by our shift-reduce algorithm
  words : List[str]
      the words that `arcs` connects

  Returns
  -------
  None
  """
  parsed=[parse_for_displacy(*reformat_dep(arcs, words))]
  renderer = DependencyRenderer()
  html = renderer.render(parsed, page=False, minify=False).strip()
  display(HTML('<span class="tex2jax_ignore">{}</span>'.format(html)))

"""Here's an example, where we try to parse the sentence "I prefer the morning flight through Denver.". Note that punctuation is treated as its own token here. Below is a brief summary of how you need to format punctuation and contractions such that they're in the same format as the training/testing data we used previously:

`"weren't" -> ["were", "n't"]`

`"Stock Market's" -> ["Stock", "Market", "'s"]`

`["'quoted text'"] -> ["``", "quoted", "text", "''"]`

`["22%"] -> ["22", "%"]`

`["$15"] -> ["$", "15"]`

Feel free to download `train.conll` or `test.conll` and take a peek at how the examples are tokenized.

Additionally, we didn't do any lowercasing on the dataset, since it actually slightly harms performance on this task (think about why it might), so also don't do lowercasing when trying out your own sentences (see that "Denver" stays capitalized).
"""

example = {'words': "I prefer the morning flight through Denver .".split(" "), 'heads': [1, None, 4, 4, 1, 6, 4, 1]}

model_arcs = model_shift_reduce(model, example, vocabulary) # get the model's parse
_, _, true_arcs = oracle_shift_reduce(example, vocabulary, is_test=True) # get the true parse

print("Predicted parse:")
display_parse(model_arcs, example['words'])
print("Actual parse:")
display_parse(true_arcs, example['words'])

"""## TODO: Conceptual question

Now, come up with some of your own examples and visualize them! Try to find some scenarios where the parses your model produces aren't as accurate, **and comment on them**. A great tool to generate the correct parses is [this site](https://corenlp.run/) by CoreNLP. Select only 'dependency parse' in the annotation selection box, type in your examples, and hit 'Submit' to see the parses!

Recall the format of the head list: The entry with value `None` is the root word, and the entries at all other words are the index of their respective heads! In the example above, 'prefer' is the root word (thus has `None` as its head), so any word in the word list who's head is 'prefer' has a `1` as its head value (since 'prefer' is the second word), and so on. You'll need to encode the correct parse given by CoreNLP into the head list, so make sure to triple-check that you got it right.

If you take a look at the kinds of words frequently used in the training dataset (there's a theme, take a look), it may explain some behaviors.

TODO:
As the example provided below:

The model struggled with dependencies involving "wherever" and "want," resulting in incorrect predictions.

Dependencies between "you" and "want" were incorrectly predicted, indicating a challenge in capturing the correct relationships in complex sentences.

Possible reasons:
Dependency parsing models may struggle with long-range dependencies, especially when a word's syntactic relationship involves non-adjacent words. Attention mechanisms or more sophisticated model architectures can help mitigate this. Also sentences with complex structures, involving multiple clauses or nested phrases, pose challenges. Improving the model's ability to handle such complexities is crucial. Variability in sentence structures, especially those not well-represented in the training data, may lead to wrong predictions. Augmenting the dataset with diverse examples can address this issue.

Improvements in handling complex sentence structures and understanding the relationships between distant words could enhance the model's performance.
"""

# TODO: code here! (feel free to make more cells.)
# Example sentence and true parses
# custom_example = {'words': "The cat chased the mouse .".split(" "), 'heads': [1, 2, None, 2, 2, 5, 2]}
# custom_example = {'words': "Can you help me with this problem ?".split(" "), 'heads': [2, 2, None, 2, 2, 6, 4, 2]}
# custom_example = {'words': "Can you help me with the dependency parsing task ?".split(" "), 'heads': [2, 2, None, 2, 4, 8, 8, 9, 5, 2]}
custom_example = {'words': "The world will take you wherever you want to go .".split(" "), 'heads': [1, 3, 3, None, 3, 4, 7, 5, 9, 7, 3]}

# Get the model's parse
model_arcs = model_shift_reduce(model, custom_example, vocabulary)
_, _, true_arcs = oracle_shift_reduce(custom_example, vocabulary, is_test=True) # get the true parse

# Display the predicted parse
print("Predicted parse:")
display_parse(model_arcs, custom_example['words'])
# the predicted parse 'heads': [1, 3, 3, None, 3, 7, 7, 3, 9, 7, 3]

# Display the actual parse
print("Actual parse:")
display_parse(true_arcs, custom_example['words'])

"""# Submission

Download this notebook as a .ipynb and .py file and submit it to Gradescope. Before doing so, additionally paste a link to your notebook in the text cell below.

**LINK TO THIS NOTEBOOK:** [...](https://)
"""