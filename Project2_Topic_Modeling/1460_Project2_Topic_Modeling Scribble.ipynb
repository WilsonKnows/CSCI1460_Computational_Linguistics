{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InwI9ewBDT6C"
      },
      "source": [
        "\n",
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ_upwLe9AEt"
      },
      "source": [
        "In this project you will leverage skills that you have learnt in the past few weeks, such as Topic Modeling, Latent Dirichlet Allocation to build a small model analyzing the content of news articles from different sources around the US.\n",
        "\n",
        "The main learning objectives for this assignment are:\n",
        "1. Use LDA topic modeling to find patterns in a realistic, noisy, unlabeled text corpus\n",
        "2. Understand how topic modeling results are influenced by preprocessing and hyperparemters\n",
        "3. Use dimensionality reduction and clustering algorithms to create effective visualizations of large text data\n",
        "\n",
        "Note: At some points in the assignment, we'll format some parts of instructions in all-caps. We're not yelling! We just think they're very important details that you must take note of.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YATqvj0RNfvD"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H7fWGJBxnzd"
      },
      "outputs": [],
      "source": [
        "# Mount the drive to be able to read and write files from your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgHtG8BlJlAF"
      },
      "outputs": [],
      "source": [
        "# Python Import Statements\n",
        "import re\n",
        "from typing import *\n",
        "import collections\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import csv\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from os.path import exists\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgxhsFaJP2JT"
      },
      "source": [
        "# Part 1: Basic Topic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXhnIPY1F-u3"
      },
      "source": [
        "First, let's train a basic topic model and see what happens! We'll do very basic text preprocessing (tokenization and lower casing) to start, and we'll use LSA for the topic model.\n",
        "\n",
        "You can download the dataset [here](https://drive.google.com/file/d/1146a1rhJ95NEySuMS000KDRPcat-itPh/view?usp=sharing) - we've handled loading it below.\n",
        "\n",
        "**Warning**: Loading the data could take as much as 10 minutes when running for the first time. After that, the parsed documents are cached in a pickle file..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKoA3r_DHurt",
        "outputId": "fdcfd407-6c75-4de5-cccd-b27ff452a806"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "501it [01:03,  8.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1001it [02:01,  9.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1502it [02:59, 12.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2000it [03:54, 14.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2501it [04:44,  7.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 2500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3001it [05:32, 15.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 3000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3502it [06:25, 12.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 3500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4000it [07:14,  8.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 4000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4501it [08:04,  7.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing row 4500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5000it [08:58,  9.28it/s]\n"
          ]
        }
      ],
      "source": [
        "######################\n",
        "#     LOAD DATA      #\n",
        "#############################################################################################################################\n",
        "# This cell block will load in the dataset into                                                                             #\n",
        "#   * spacy_processed_docs - A list of Spacy Documents that we can use as data to train our Topic Model                     #\n",
        "#   * doc_locations - A list of States where each Document was sourced from (This will be used in Part 3: Visualization)    #\n",
        "#############################################################################################################################\n",
        "\n",
        "num_documents = 5000 # INFO: Feel free to change this to load in less documents for debugging but otherwise keep it at 5000 to train the Topic Model\n",
        "# FILEPATH = \"path/to/dataset\" # TODO: Update this to the filepath of your copy of the assignment, e.g. /content/drive/MyDrive/Topic Modeling/\n",
        "FILEPATH = \"\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc_locations = []\n",
        "spacy_processed_docs = []\n",
        "\n",
        "if exists(f\"{FILEPATH}spacy_processed_docs_{num_documents}.pkl\"):\n",
        "  with open(f\"{FILEPATH}spacy_processed_docs_{num_documents}.pkl\", 'rb') as f:\n",
        "    spacy_processed_docs, doc_locations = pickle.load(f)\n",
        "    f.close()\n",
        "else:\n",
        "  with open(f'{FILEPATH}articles_sampled_data.csv', 'r', encoding='utf-8') as f:\n",
        "    for i, row in tqdm(enumerate(csv.DictReader(f, delimiter=','))):\n",
        "      if i == num_documents:\n",
        "        break\n",
        "      if i % 500 == 0:\n",
        "        print(\"Processing row %d\"%i)\n",
        "      try:\n",
        "        parsed = nlp(row[\"content\"])\n",
        "        source_name = row[\"location\"]\n",
        "      except ValueError:\n",
        "        continue\n",
        "      spacy_processed_docs.append(parsed)\n",
        "      doc_locations.append(source_name)\n",
        "    f.close()\n",
        "\n",
        "  with open(f\"{FILEPATH}spacy_processed_docs_{num_documents}.pkl\", 'wb') as f:\n",
        "    pickle.dump((spacy_processed_docs, doc_locations), f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WooUMNPEBzi3",
        "outputId": "9bf57844-b38e-4c3b-872e-3a9728686597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROCESSED\n",
            "By Special to the Advance\n",
            "\n",
            "Spain Park’s Sarah Ashlee Barker is the Alabama Sports Writers Association 33rd Miss Basketball and first from Spain Park.\n",
            "\n",
            "The Georgia signee is also the Gatorade state Player of the Year, the MaxPreps state Player of the Year, the AL.com Player of the Year and the AL.com Birmingham Region Player of the Year.\n",
            "\n",
            "She has been selected to the Alabama Sports Writers Association All-State team each of the past three seasons and is the two-time Class 7A Player of the Year.\n",
            "\n",
            "Barker led the Jaguars to their second Class 7A state championship in the past three seasons, averaging 23.3 points, 10.9 rebounds and 2.7 assists this season for coach Mike Chase.\n",
            "\n",
            "She earned Class 7A tournament MVP this season and was also named all-tournament on Spain Park’s 2018 championship team and 2017 finalist squad.\n",
            "\n",
            "Number of documents: 5000\n"
          ]
        }
      ],
      "source": [
        "# Run this to make sure that the data is loaded in correctly, there should be 5000 documents\n",
        "print(\"PROCESSED\")\n",
        "print(spacy_processed_docs[0])\n",
        "print(f\"\\nNumber of documents: {len(spacy_processed_docs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj4JoyFoZqdl"
      },
      "source": [
        "To start, let's build the following components for topic modeling:\n",
        "\n",
        "1.  **M**:\n",
        "    A binary term-document matrix of shape (num_documents, vocab_size)\n",
        "\n",
        "2.  **word2idx**: A dictionary which maps each word to its rank in the vocabulary (e.g. the most frequent word should have rank 0, the second most frequent word rank 1, etc).\n",
        "\n",
        "3.  **idx2word**: The inverse of the above (i.e., mapping from index to word)\n",
        "\n",
        "You will use **M** to train the topic model directly, and you will use the other two lookup tables in order to analyze the actual topics produced. You should use spacy to ***TOKENIZE*** and ***LOWERCASE*** the raw text, but not do any additional preprocessing.\n",
        "When lowercasing, please use `token.lemma_.lower()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "4SNQ0gICChRd",
        "outputId": "45f1229f-5e14-42cc-c956-831c5d8fd0f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-87e253679585>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mbinary_term_doc_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \"\"\"\n\u001b[1;32m      3\u001b[0m   \u001b[0mPreprocess\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcreate\u001b[0m \u001b[0mour\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdocument\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionaries\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdescribed\u001b[0m \u001b[0mabove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mTODO\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mYou\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfill\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mHINT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMake\u001b[0m \u001b[0msure\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mto\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mrather\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
          ]
        }
      ],
      "source": [
        "def binary_term_doc_matrix(docs : List[spacy.tokens.Doc]) -> Tuple[np.ndarray[np.float64], Dict[int, str]]:\n",
        "  \"\"\"\n",
        "  Preprocess and transform docs to create our binary term-document matrix and dictionaries as described above\n",
        "  TODO: You will need to fill in word2idx and idx2word, and then M\n",
        "  HINT: Make sure to use token.lemma_.lower() to lowercase text rather than token.text.lower()\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  docs : List[Doc]\n",
        "    A list of spacy processed documents (i.e. each item is the output of nlp(article))\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  M : np.ndarray[float]\n",
        "    The binary term-document matrix, each value in M should be either 0 or 1\n",
        "  idx2word : Dict[int, str]\n",
        "    The dictionary that maps each index/rank to each word in the vocabulary\n",
        "  \"\"\"\n",
        "\n",
        "  # # Create a vocabulary by iterating through all documents\n",
        "  # vocab = {}\n",
        "  # for doc in docs:\n",
        "  #   for token in doc:\n",
        "  #     word = token.lemma_.lower()\n",
        "  #     if word not in vocab:\n",
        "  #       vocab[word] = 1\n",
        "  #     else:\n",
        "  #       vocab[word] += 1\n",
        "\n",
        "  # # Sort the vocabulary by word frequency in descending order\n",
        "  # sorted_vocab = sorted(vocab.items(), key=lambda word: word[1], reverse=True)\n",
        "\n",
        "  # # Create word-to-index and index-to-word dictionaries\n",
        "  # word2idx = {word: index for index, (word, _) in enumerate(sorted_vocab)}\n",
        "  # idx2word = {index: word for word, index in word2idx.items()}\n",
        "\n",
        "  # # Initialize the binary term-document matrix M\n",
        "  # M = np.zeros((len(docs), len(vocab)))\n",
        "\n",
        "  # # Fill in the binary term-document matrix\n",
        "  # for doc_idx, doc in enumerate(docs):\n",
        "  #   for token in doc:\n",
        "  #     word = token.lemma_.lower()\n",
        "  #     if word in word2idx:\n",
        "  #       word_idx = word2idx[word]\n",
        "  #       M[doc_idx, word_idx] = 1\n",
        "\n",
        "  # return M, idx2word\n",
        "\n",
        "  # Create a vocabulary of unique lemmatized and lowercase words\n",
        "  vocabulary = set()\n",
        "\n",
        "  for doc in docs:\n",
        "      for token in doc:\n",
        "          vocabulary.add(token.lemma_.lower())\n",
        "\n",
        "  # Create word2idx and idx2word dictionaries\n",
        "  word2idx = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
        "  idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "  # Initialize the binary term-document matrix M\n",
        "  M = np.zeros((len(docs), len(vocabulary)))\n",
        "\n",
        "  # Fill in the binary term-document matrix M\n",
        "  for doc_idx, doc in enumerate(docs):\n",
        "      for token in doc:\n",
        "          word = token.lemma_.lower()\n",
        "          word_idx = word2idx[word]\n",
        "          M[doc_idx, word_idx] = 1\n",
        "\n",
        "  return M, idx2word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YmP2t3MVFmQi",
        "outputId": "c92e5f55-6a73-4d4e-bfdb-21042804daa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5000, 59205)\n"
          ]
        }
      ],
      "source": [
        "# Run the function you just wrote, the shape should be (5000, 59205)\n",
        "###############################################\n",
        "M,idx2word = binary_term_doc_matrix(spacy_processed_docs)\n",
        "print(M.shape)\n",
        "###############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZuYpoZvL0BYM"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def train_topic_model(term_doc_mat : np.ndarray[np.float64], n_topics : int = 10, random_state = 42) -> LatentDirichletAllocation:\n",
        "  \"\"\"\n",
        "  Train a n_topics topic model on M using Latent Dirichlet Allocation\n",
        "  TODO: Use LDA to fit a model with n_topics, then return the model\n",
        "  NOTE: You MUST use the given random state when intialzing your LDA model.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  term_doc_mat : np.ndarray[float]\n",
        "    The term-document matrix to train the LDA model on\n",
        "  n_topics : int\n",
        "    The number of topics in the topic model (Defaulted to 10)\n",
        "  random_state : int\n",
        "    The random state of the LDA Model (Defaulted to 42)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  lda : LatentDirichletAllocation\n",
        "    The trained LDA model\n",
        "  \"\"\"\n",
        "  # Create an instance of the LDA model\n",
        "  lda = LatentDirichletAllocation(n_components=n_topics, random_state=random_state)\n",
        "\n",
        "  # Fit the LDA model to the term-document matrix\n",
        "  lda.fit(term_doc_mat)\n",
        "\n",
        "  return lda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LRaN2fLrKPlt"
      },
      "outputs": [],
      "source": [
        "def preview_topics(topic_model: LatentDirichletAllocation, idx2word: Dict[int, str]) -> List[List[str]]:\n",
        "  \"\"\"\n",
        "  TODO: Print out/return the top 10 words associated with each topic\n",
        "  HINT: You will need to use idx2word and will likely find numpy's argsort to be helpful here\n",
        "        Make sure you check the sklearn documentation to get each topic from the model\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  topic_model : LatentDirichletAllocation\n",
        "    The trained LDA Topic Model\n",
        "  idx2word : Dict[int, str]\n",
        "    The dictionary that maps each index/rank to each word in the vocabulary\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  topics : List[List[str]]\n",
        "    A list of the 10 words associated with each topic\n",
        "  \"\"\"\n",
        "  topics = []\n",
        "\n",
        "  # Loop through each topic in the model\n",
        "  for topic_idx, topic in enumerate(topic_model.components_):\n",
        "      # Get the indices that would sort the topic words by their weights in descending order\n",
        "      sorted_word_indices = topic.argsort()[::-1]\n",
        "\n",
        "      # Get the top 10 words associated with this topic\n",
        "      top_words = [idx2word[word_idx] for word_idx in sorted_word_indices[:10]]\n",
        "\n",
        "      # Append the top words for this topic\n",
        "      topics.append(top_words)\n",
        "\n",
        "  return topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DWwYytS4NvcK",
        "outputId": "16526da0-cb4c-412a-d22b-f21a6894204c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['the', '.', ',', 'in', 'and', 'of', 'to', '-', 'for', 'be'],\n",
              " ['the', 'to', ',', 'in', 'be', 'of', '.', 'and', 'for', 'a'],\n",
              " ['en', 'del', 'por', 'para', 'según', 'una', 'un', 'que', 'y', 'con'],\n",
              " [',', '.', 'be', 'of', 'at', 'in', 'funeral', 'and', 'bear', 'service'],\n",
              " ['robles',\n",
              "  'atkins',\n",
              "  'grand',\n",
              "  '120',\n",
              "  'king',\n",
              "  'zunino',\n",
              "  'machine',\n",
              "  'david',\n",
              "  'labs',\n",
              "  'john'],\n",
              " ['the', 'and', 'be', '.', ',', 'of', 'to', 'a', '\\n\\n', 'in'],\n",
              " ['i', 'do', 'it', 'can', 'you', 'not', 'that', 'we', 'what', 'but'],\n",
              " ['the', '.', ',', 'a', 'of', 'and', 'be', 'in', 'on', 'to'],\n",
              " ['.', 'the', 'to', ',', 'be', '\\n\\n', 'a', 'of', 'and', 'in'],\n",
              " ['.', 'the', ',', 'be', 'to', 'and', 'in', '\\n\\n', 'a', 'of']]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now run it and let's look at the output!\n",
        "###############################################\n",
        "topic_model = train_topic_model(M, n_topics=10)\n",
        "preview_topics(topic_model, idx2word)\n",
        "\n",
        "#1m29s to run\n",
        "###############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnemfJRULgEm"
      },
      "source": [
        "# Part 2: Improved Topic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj0mvEkoLjdU"
      },
      "source": [
        "Now let's try to improve the quality of the topics by improving the features our model can use (as opposed to binary features).\n",
        "\n",
        "1. First, we'll write a helper function to preprocess a spacy document. This is similar to what you did in project 1. Here, we only want you to ***LOWERCASE, REMOVE NEWLINES*** `(token.pos_=='SPACE')`, ***REMOVE PUNCTUATION***, and ***REMOVE STOPWORDS***.\n",
        "\n",
        "\n",
        "2.  Then, you will implement TFIDF\n",
        "\n",
        "\n",
        "TFIDF is the product of two statistics:\n",
        "\n",
        "1. **Term Frequency (tf)**: The relative frequency of a term $w$ in a document $d$. We will use the following formula:\n",
        "  \n",
        "$$\n",
        "    tf = 0.5 + \\left(0.5 \\cdot \\frac{f_w}{\\max\\{f_w' : w' \\in d\\}}\\right)\n",
        "$$\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where $f_w$ is the frequency $w$ in document $d$.\n",
        "  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each document, each word in that document has a tf value. For words not in that document ***the tf value should be 0 NOT 0.5***\n",
        "\n",
        "  \n",
        "\n",
        "2. **Inverse Document Frequency (idf)**:  For the whole corpus of documents $D$, how many of the documents does the term $w$ appear?        \n",
        "\n",
        "  Intuitively, this is how much information a word provides if it appears in a document.\n",
        "  We will use the following formula:\n",
        "\n",
        "$$\n",
        "    idf = log(\\frac{|D|}{|d\\ \\in D: w\\ \\in d|})\n",
        "$$\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where the denominator indicates the number of documents that a term $w$ appears in.\n",
        "  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Note that there is one idf value for each word in the vocab.\n",
        "  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hint: You will have to iterate over all of the documents to calculate idf prior to remaking your matrix M\n",
        "\n",
        "Finally, the TFIDF score is calculated by multiplying these two values together:\n",
        "$$\n",
        "    TFIDF = tf * idf\n",
        "$$\n",
        "\n",
        "With this in mind, Let's reconstruct **M**, **word2idx**, and **idx2word**.\n",
        "This time, let's make the following changes:\n",
        "1. Filter out ***STOPWORDS***, ***PUNCTUATION***, ***NEWLINES*** and make everything ***LOWERCASE*** using spacy\n",
        "2. Filter the vocabulary to only the ***MOST FREQUENT*** 5000 words\n",
        "3. Use TFIDF values instead of binary count to remake **M**\n",
        "\n",
        "Hint: although it's a little less efficient, it might be easier to iterate through the list of documents multiple times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv_-YgOjGPO8"
      },
      "outputs": [],
      "source": [
        "from collections import Counter # HINT: you may find this useful\n",
        "import math\n",
        "\n",
        "def preprocess_doc(doc : spacy.tokens.Doc) -> List[str]:\n",
        "  \"\"\"\n",
        "  TODO: Write a helper function that filters out STOPWORDS, PUNCTUATION, and NEWLINES from the spacy processed doc.\n",
        "        Also LOWERCASE the tokens as well.\n",
        "  HINT: Make sure to use token.lemma_.lower() to lowercase text rather than token.text.lower()\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  doc : Doc\n",
        "    The spacy preprocessed document\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  preprocessed : List[Str]\n",
        "    A list of the preprocessed strings\n",
        "  \"\"\"\n",
        "  proc_doc = []\n",
        "  for token in doc:\n",
        "      # Check if the token is not a newline, not punctuation, and not a stop word\n",
        "      if token.pos_!='SPACE' and not token.is_space and not token.is_punct and not token.is_stop:\n",
        "          proc_doc.append(token.lemma_.lower())\n",
        "  return proc_doc\n",
        "\n",
        "def create_vocab(proc_docs : List[List[str]], vocab_cutoff : int = 5000) -> List[str]:\n",
        "  \"\"\"\n",
        "  Aggregates and collects the text of the most common tokens in docs, cutoff by the vocab_cutoff.\n",
        "  Hint: You may find Counter() useful here\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  proc_docs : List[List[str]]\n",
        "    A list of preprocessed documents using preprocess_doc\n",
        "  vocab_cutoff : int\n",
        "    The cutoff of the MOST FREQUENT vocabulary\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  vocab : List[Str]\n",
        "    A list of the top {vocab_cutoff} most common token texts in docs\n",
        "  \"\"\"\n",
        "  # Flatten the list of preprocessed documents\n",
        "  all_words = [word for doc in proc_docs for word in doc]\n",
        "\n",
        "  # Use Counter to count the frequency of each word\n",
        "  word_counts = Counter(all_words)\n",
        "\n",
        "  # Get the top 5000 most common words\n",
        "  vocab = [word for word, count in word_counts.most_common(vocab_cutoff)]\n",
        "\n",
        "  return vocab\n",
        "\n",
        "def idf_matrix(proc_docs : List[List[str]], word2idx : Dict[str, int],  vocab : List[str]) -> np.ndarray[np.float64]:\n",
        "  \"\"\"\n",
        "  Calculate the Inverse Document Frequency (IDF) Matrix using the equation above for each word\n",
        "  Equation: idf(w) = log(|D| / |d in D : w in d|)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  proc_docs : List[List[str]]\n",
        "    A list of preprocessed documents using preprocess_doc\n",
        "  word2idx : Dict[Str, Int]\n",
        "    A dictionary that matches each word in the vocabulary to it's rank\n",
        "  vocab : List[Str]\n",
        "    The actual vocab of all the docs (thresholded by vocab_cutoff)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  idf : np.array[Float]\n",
        "    The IDF array as defined by the equation in the description\n",
        "  \"\"\"\n",
        "  idf = np.zeros(len(vocab))\n",
        "\n",
        "  # Count the number of documents in which each word appears\n",
        "  doc_word_count = np.zeros(len(vocab))\n",
        "\n",
        "  # HINT: It may be useful to start by counting the number of documents a word shows up in for each word\n",
        "  for doc in proc_docs:\n",
        "    # Track unique words within a document\n",
        "    unique_words_in_doc = set(doc)\n",
        "    for word in unique_words_in_doc:\n",
        "      if word in word2idx:\n",
        "        word_idx = word2idx[word]\n",
        "        doc_word_count[word_idx] += 1\n",
        "\n",
        "  # Calculate IDF for each word in the vocabulary\n",
        "  for word_idx, word in enumerate(vocab):\n",
        "    if doc_word_count[word_idx] > 0:\n",
        "      idf[word_idx] = np.log(len(proc_docs) / doc_word_count[word_idx])\n",
        "\n",
        "  return idf\n",
        "\n",
        "def tf_matrix(proc_docs : List[List[str]], word2idx : Dict[str, int], vocab : List[str]) -> np.ndarray[np.float64]:\n",
        "  \"\"\"\n",
        "  Calculate the Term Frequency (TF) Matrix using the equation above for each word\n",
        "  Equation: tf(w, d) = 0.5 + 0.5 * (freq_w_in_d / freq_wmax_in_d)\n",
        "  NOTE: For words not in the document the TF value should be 0 and NOT 0.5\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  proc_docs : List[List[str]]\n",
        "    A list of preprocessed documents using preprocess_doc\n",
        "  word2idx : Dict[Str, Int]\n",
        "    A dictionary that matches each word in the vocabulary to it's rank\n",
        "  vocab : List[Str]\n",
        "    The actual vocab of all the docs (thresholded by vocab_cutoff)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tf : np.array[Float]\n",
        "    The TF array as defined by the equation in the description\n",
        "  \"\"\"\n",
        "  tf = np.zeros((len(proc_docs), len(vocab)))\n",
        "\n",
        "  for doc_idx, doc in enumerate(proc_docs):\n",
        "    doc_word_counts = {}  # Store word frequencies in the document\n",
        "    freq_wmax_in_d = 0  # Find the maximum word frequency in the document\n",
        "\n",
        "    # Count word frequencies in the document, find the maximum frequency\n",
        "    for word in doc:\n",
        "      if word in word2idx:\n",
        "        word_idx = word2idx[word]\n",
        "        if word_idx in doc_word_counts:\n",
        "          doc_word_counts[word_idx] += 1\n",
        "        else:\n",
        "          doc_word_counts[word_idx] = 1\n",
        "        freq_wmax_in_d = max(freq_wmax_in_d, doc_word_counts[word_idx])\n",
        "\n",
        "    # Calculate TF values for each word in the vocab\n",
        "    for word_idx, word in enumerate(vocab):\n",
        "      if word_idx in doc_word_counts:\n",
        "        if freq_wmax_in_d != 0:  # Check for division by zero\n",
        "          tf_value = 0.5 + 0.5 * (doc_word_counts[word_idx] / freq_wmax_in_d)\n",
        "          tf[doc_idx, word_idx] = tf_value\n",
        "        else:\n",
        "          tf[doc_idx, word_idx] = 0.0  # Handle division by zero case\n",
        "\n",
        "  return tf\n",
        "\n",
        "\n",
        "def tfidf_term_doc_matrix(docs : List[spacy.tokens.Doc], vocab_cutoff : int = 5000) -> Tuple[np.ndarray[np.float64], Dict[int, str]]:\n",
        "  \"\"\"\n",
        "  There are multiple steps in this function:\n",
        "  TODO:\n",
        "    1. Create the vocab\n",
        "    2. Threshold it by vocab_cutoff and compute IDF\n",
        "    3. Compute TF for each word in the document\n",
        "    4. Use TF and IDF to calculate TFIDF for each entry in M\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  docs : List[Doc]\n",
        "    A list of spacy preprocessed documents\n",
        "  vocab_cutoff : int\n",
        "    The cutoff of the MOST FREQUENT vocabulary\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  M : np.ndarray[float]\n",
        "    The TFIDF term document Matrix\n",
        "  idx2word : Dict[int, str]\n",
        "    The dictionary that maps each index/rank to each word in the vocabulary\n",
        "  \"\"\"\n",
        "\n",
        "  word2idx = {}\n",
        "  idx2word = {}\n",
        "  proc_docs = [] # A list of preprocessed docs\n",
        "\n",
        "  # TODO: Preprocess each document and compute your thresholded vocab\n",
        "  for doc in docs:\n",
        "    proc_doc = preprocess_doc(doc)\n",
        "    proc_docs.append(proc_doc)\n",
        "\n",
        "  # Sort the vocabulary by word frequency in descending order\n",
        "  vocab = create_vocab(proc_docs, vocab_cutoff)\n",
        "  M = np.zeros((len(proc_docs), len(vocab)))\n",
        "  # TODO: Fill in word2idx and idx2word\n",
        "  # Create word-to-index and index-to-word dictionaries\n",
        "  word2idx = {word: index for index, word in enumerate(vocab)}\n",
        "  idx2word = {index: word for word, index in word2idx.items()}\n",
        "\n",
        "  # TODO: Calculate the IDF array\n",
        "  idf = idf_matrix(proc_docs, word2idx, vocab)\n",
        "\n",
        "  # TODO: Calculate the TF array\n",
        "  tf = tf_matrix(proc_docs, word2idx, vocab)\n",
        "\n",
        "  # TODO: Combine the TF and IDF array to make the TFIDF array (M)\n",
        "  M = tf * idf # tf @ idf\n",
        "\n",
        "  #Do not modify the return statement\n",
        "  return M, idx2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nBA_poNLsdl",
        "outputId": "205940af-e7ea-49d5-a274-b02832adebc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['event',\n",
              "  'thing',\n",
              "  'life',\n",
              "  'learn',\n",
              "  'live',\n",
              "  'like',\n",
              "  'experience',\n",
              "  'way',\n",
              "  'want',\n",
              "  'offer'],\n",
              " ['pitch',\n",
              "  'inning',\n",
              "  'baseball',\n",
              "  'bat',\n",
              "  'hit',\n",
              "  'pitcher',\n",
              "  'game',\n",
              "  'homer',\n",
              "  'rbi',\n",
              "  'innings'],\n",
              " ['water',\n",
              "  'weather',\n",
              "  'company',\n",
              "  'area',\n",
              "  'mile',\n",
              "  'wind',\n",
              "  'storm',\n",
              "  'near',\n",
              "  'gas',\n",
              "  'damage'],\n",
              " ['click',\n",
              "  'subscription',\n",
              "  'subscriber',\n",
              "  'content',\n",
              "  'login',\n",
              "  'access',\n",
              "  'register',\n",
              "  'log',\n",
              "  'premium',\n",
              "  'site'],\n",
              " ['$',\n",
              "  'fund',\n",
              "  'board',\n",
              "  'meeting',\n",
              "  'council',\n",
              "  'approve',\n",
              "  'district',\n",
              "  'business',\n",
              "  'tax',\n",
              "  'money'],\n",
              " ['church',\n",
              "  'daughter',\n",
              "  'son',\n",
              "  'wife',\n",
              "  'funeral',\n",
              "  'sister',\n",
              "  'brother',\n",
              "  'husband',\n",
              "  'bear',\n",
              "  'family'],\n",
              " ['health',\n",
              "  'virus',\n",
              "  'test',\n",
              "  'covid-19',\n",
              "  'case',\n",
              "  'patient',\n",
              "  'coronavirus',\n",
              "  'disease',\n",
              "  'positive',\n",
              "  'hospital'],\n",
              " ['trump',\n",
              "  'election',\n",
              "  'president',\n",
              "  'biden',\n",
              "  'political',\n",
              "  'republican',\n",
              "  'democratic',\n",
              "  'donald',\n",
              "  'protest',\n",
              "  'campaign'],\n",
              " ['police',\n",
              "  'arrest',\n",
              "  'charge',\n",
              "  'officer',\n",
              "  'sheriff',\n",
              "  'court',\n",
              "  'vehicle',\n",
              "  'man',\n",
              "  'p.m.',\n",
              "  'attorney'],\n",
              " ['game',\n",
              "  'coach',\n",
              "  'team',\n",
              "  'season',\n",
              "  'player',\n",
              "  'play',\n",
              "  'win',\n",
              "  'football',\n",
              "  'championship',\n",
              "  'score']]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now try see what happens when you train a topic model with preprocessed text and tfidf features!\n",
        "###############################################################\n",
        "M, idx2word = tfidf_term_doc_matrix(spacy_processed_docs)\n",
        "\n",
        "topic_model = train_topic_model(M, n_topics=10)\n",
        "preview_topics(topic_model, idx2word)\n",
        "###############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_wDw1pYZqdm"
      },
      "source": [
        "Now let's tune the number of topics in order to determine the \"right\" number of topics.\n",
        "\n",
        "To do this, we'll choose the number of topics that minimizes perplexity on held-out data. Specifically, you will need to do the following:\n",
        "1. Split your data into 80% train and 20% dev\n",
        "2. Using the training data, train 5 topic models, one for each of the following numbers of topics: [1, 5, 10, 15, 20]\n",
        "3. For each trained model, compute the perplexity on the dev set, and plot the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrE03TJPLifF"
      },
      "outputs": [],
      "source": [
        "def test_models(M : np.ndarray[np.float64], ks : List[int], dev_split : int = 0.2):\n",
        "  \"\"\"\n",
        "  Tests out your models on multiple different values of k, where k is the number of topics for your model\n",
        "  Then it returns a perplexity list for each value of k\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  M : np.ndarray[float]\n",
        "    The input term document Matrix\n",
        "  ks : List[int]\n",
        "    A list of different k-values (numbers of topics) to test on\n",
        "  dev_split : int\n",
        "    How much you want to split your data for testing. In our case dev_split should be 0.2 so that we split\n",
        "    our data into 80% train and 20% dev.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  train_scores : List[float]\n",
        "    The perplexity scores of each model in training\n",
        "  dev_scores : List[float]\n",
        "    The perplexity scores of each model in dev\n",
        "  \"\"\"\n",
        "  train_data, dev_data = train_test_split(M, test_size=dev_split)\n",
        "\n",
        "  train_scores = []\n",
        "  dev_scores = []\n",
        "  for k in ks:\n",
        "    print(\"Training LDA model with %d topics...\"%k)\n",
        "    model = train_topic_model(M, k)\n",
        "    train_scores.append(model.perplexity(train_data))\n",
        "    dev_scores.append(model.perplexity(dev_data))\n",
        "\n",
        "  return train_scores, dev_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "amY4EZkFuAz-",
        "outputId": "8ff120fb-aca8-4f09-9278-18b44ec5bd0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LDA model with 1 topics...\n",
            "Training LDA model with 5 topics...\n",
            "Training LDA model with 10 topics...\n",
            "Training LDA model with 15 topics...\n",
            "Training LDA model with 20 topics...\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the perplexity of each topic model\n",
        "ks = [1, 5, 10, 15, 20]\n",
        "train_scores, dev_scores = test_models(M, ks)\n",
        "plt.plot(np.arange(len(train_scores)), train_scores, label=\"Train\")\n",
        "plt.plot(np.arange(len(dev_scores)), dev_scores, label=\"Dev\")\n",
        "plt.xticks(np.arange(len(ks)), ['%d'%k for k in ks])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxNzeBZ0gxmj"
      },
      "source": [
        "# Part 3: Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQjnCFsVXCSI"
      },
      "source": [
        "Now, for the exciting part! Let's use the topic model you just trained to inspect the data visually.\n",
        "\n",
        "First, Let's train the model with the amount of topics that minimized perplexity above..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt-e8ua9jFhB"
      },
      "outputs": [],
      "source": [
        "# TODO: Train the model on the full data using the number of topics you chose above (i.e., the value of k that minimized perplexity on dev).\n",
        "\n",
        "best_k = 20 # ??? # The number of Topics that got the best perplexity\n",
        "topic_model = train_topic_model(M, best_k) # ??? # Train the topic model with best_k topics\n",
        "topics = preview_topics(topic_model, idx2word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wns43Z6Jv5yW"
      },
      "source": [
        "To start, we want to visualize our Term Document Matrix, but that can be difficult to visualize because each document is represented by a 5000 component vector. Thus in order to visualize our Matrix, we will want to use **dimensionality reduction** to reduce the number of components to 2 so that we can plot each Document on a 2D grid.  \n",
        "\n",
        "In the cell below, we'll make use of PCA (Principle Component Analysis), a process that uses SVD (Singular Value Decomposition) to extract out the 2 most important Principal Components of our data which we can use to visualize our Matrix. You will NOT need to code this process from scratch, instead we'll make use of `sklearn.decomposition.PCA` to do it for you.\n",
        "\n",
        "- The output of `PCA.fit_transform()` is a matrix of shape `(n_samples, n_components)`, where `n_samples = #documents`, and `n_components = 2`, in our case. We will treat each row `i` of the output as 2D coordinates for document `i`.\n",
        "- Check out the PCA documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UAbckGOCp2s"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# NOTE: This code initialized our PCA model to project our matrix M into 2 dimensions\n",
        "def pca_projection(M : np.ndarray[np.float64], n_components : int = 2) -> np.ndarray[np.float64]:\n",
        "  \"\"\"\n",
        "  Use PCA to project M into n_components.\n",
        "  Note: You do NOT need to code this function.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  M : np.ndarray[float]\n",
        "    The input Term Document Matrix\n",
        "  n_components : int\n",
        "    The number of components/dimensions that you want to reduce to\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  proj : np.ndarray[float]\n",
        "    The projected matrix (M projected to n_components)\n",
        "  \"\"\"\n",
        "  pca = PCA(n_components) # PCA model with n_components = 2\n",
        "  proj = pca.fit_transform(M) # Projections: An np.array of shape (n_samples, n_components)\n",
        "  return proj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT5d2sr3Zqdn"
      },
      "source": [
        "With this projection we can now plot all of the documents on a 2d plane, but ideally we want each point to be colored by their \"most prominent Topic\" and contain some more identifying data about the document.\n",
        "\n",
        "TODO: In order to achieve this, for each point lets create a function to build a dataframe **df** such that\n",
        "1. Each row corresponds to one of the documents\n",
        "2. The **topic** column maps each Document to it's \"Most Prominent Topic\"\n",
        "3. The **text** column maps each Document to the a quick text snippet from the Document (the first 100 characters of the Document's raw text)\n",
        "4. The **x** and **y** columns map to the (x, y) coordinates of the 2d projection\n",
        "\n",
        "To help you start we have created an empty dataframe with the columns defined in but with no data. Please fill in this dataframe with information from **docs**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq-Dz-c7Zqdn"
      },
      "outputs": [],
      "source": [
        "def generate_datapoints(docs : List[spacy.tokens.Doc], proj: np.ndarray[np.float64], M : np.ndarray[np.float64], topic_model: LatentDirichletAllocation) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Generates a Pandas Dataframe where each row corresponds to the data point (document). Specifically:\n",
        "    1. The (x, y) coordinates of the projected document\n",
        "    2. The text of the document (capped at 100 characters)\n",
        "    3. The most prominent topic of the document (As a string of the topic number e.g. \"2\")\n",
        "  An empty Dataframe with the required columns is already set up for you. Please fill it in and return it.\n",
        "  To help, the related topic_scores and projections for each document are already calculated for you!\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  docs : List[Doc]\n",
        "    A list of spacy preprocessed documents\n",
        "  M : np.ndarray[float]\n",
        "    The input Term Document Matrix\n",
        "  proj: np.ndarray[float]\n",
        "    The projection matrix which is M projected to 2d space using our previously defined pca_projection function\n",
        "  topic_model: LatentDirichletAllocation\n",
        "    The trained topic model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  df : DataFrame\n",
        "    The filled out Dataframe\n",
        "  \"\"\"\n",
        "  df = pd.DataFrame(columns = ['topic', 'text', 'x', 'y'])\n",
        "\n",
        "  # HINT: To help you find the \"Most Prominent Topic\" for each document, we can use our topic model to transform M so that\n",
        "  #       we get a matrix of shape (num_documents, num_topic), where each entry is the \"Prominence\" score of that topic for that document.\n",
        "  # Calculate the most prominent topic for each document\n",
        "  topic_scores = topic_model.transform(M)\n",
        "  most_prominent_topics = np.argmax(topic_scores, axis=1)\n",
        "  # Create a list of dictionaries to store data for each document\n",
        "  doc_data_list = []\n",
        "\n",
        "  # TODO: Fill out the dataframe with all of the columns filled out for each document (row).\n",
        "  #       You're free to implement this however you wish (please see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
        "  #       However it might be easiest to first create a list of dictionaries and then call the pd.DataFrame constructor on the list\n",
        "  for index, doc in enumerate(docs):\n",
        "      # Extract the (x, y) coordinates from the projection matrix\n",
        "      x, y = proj[index]\n",
        "\n",
        "      # Get the most prominent topic as a string\n",
        "      most_prominent_topic = str(most_prominent_topics[index])\n",
        "\n",
        "      # Extract the first 100 characters of the document's raw text as text snippet\n",
        "      text = doc.text[:100]\n",
        "\n",
        "      # Create a dictionary for the current document and append it to the DataFrame\n",
        "      doc_data = {'topic': most_prominent_topic, 'text': text, 'x': x, 'y': y}\n",
        "      # Append the document data to the list\n",
        "      doc_data_list.append(doc_data)\n",
        "\n",
        "  # Concatenate the list of dictionaries into a DataFrame\n",
        "  df = pd.concat([df, pd.DataFrame(doc_data_list)], ignore_index=True)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjy1ABiwHeV"
      },
      "source": [
        "Now, run the following cell to plot the PCA projection of our documents onto 2D! Each point corresponds to a document, which you can hover over and view some of the contents of!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biI7bKE4YNGr"
      },
      "outputs": [],
      "source": [
        "# This plots the plot_data list built in the previous cell, nothing to do here\n",
        "df = generate_datapoints(spacy_processed_docs, pca_projection(M, 2), M, topic_model)\n",
        "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"topic\", hover_data=['text'])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7X2t8rXbc96"
      },
      "source": [
        "## Geographical Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmckDyRwUmN"
      },
      "source": [
        "Now, we'd like you to visualize the most popular topics based on geographical location!\n",
        "\n",
        "We'll start with a simple bar graph showing the popularity of topics by state, where the height of each bar is the proportion of documents from each location that correspond to a particular topic.\n",
        "\n",
        "As a warning, these counts of documents are unnormalized... Because of this, states that have a lot of documents may more regularly appear at the front of the bar charts. These charts don't indicate which topics are proportionally popular by state, despite it being more intuitive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO3kVDI5Zqdn"
      },
      "source": [
        "To visualize the most popular counts by location and topic we first need to group and count the number of documents that have that \"most prominent topic\" and who originate from that location.\n",
        "\n",
        "TODO: Please fill in the `generate_location_data` function which takes in a list of locations and documents and creates a Dictionary that maps each topic to a Dictionary of {Location : Count}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52SDPvZqZqdn"
      },
      "outputs": [],
      "source": [
        "def generate_location_data(docs : List[spacy.tokens.Doc], M : np.ndarray[np.float64], locations : List[str], topic_model: LatentDirichletAllocation) -> Dict[int, Dict[str, int]]:\n",
        "  \"\"\"\n",
        "  Aggregates the Documents that all share the same \"Most Prominent Topic\" and retains a count of the number of Documents at each location\n",
        "  for each Document. This will be used to generate a Bar Chart visualization and Map Visualization of where each Topic is most popular.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  docs : List[Doc]\n",
        "    A list of spacy preprocessed documents\n",
        "  M : np.ndarray[float]\n",
        "    The input Term Document Matrix\n",
        "  locations : List[str]\n",
        "    A list that records the location of each document. Thus, the location of i-th document is doc_locations[i]\n",
        "  topic_model: LatentDirichletAllocation\n",
        "    The trained topic model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  location_count : Dict[int, Dict[str, int]]\n",
        "    A dictionary that maps each topic (int) to a Dictionary mapping Location (str) to Counts (int) for that topic.\n",
        "    This is designed so that\n",
        "      * location_count[topic][state] is the count of documents for that state and topic\n",
        "  \"\"\"\n",
        "  location_count = {}\n",
        "\n",
        "  # HINT: To help you find the \"Most Prominent Topic\" for each document, we can use our topic model to transform M so that\n",
        "  #       we get a matrix of shape (num_documents, num_topic), where each entry is the \"Prominence\" score of that topic for that document.\n",
        "  topic_scores = topic_model.transform(M)\n",
        "\n",
        "  # Hint: It may be helpful to first create a dictionary mapping Topics (int) to a List of Locations\n",
        "  #       of the documents who have the Topic as it's \"Most Prominent Topic\"\n",
        "  # Loop through each document\n",
        "  for i, doc in enumerate(docs):\n",
        "      # Get the most prominent topic for this document\n",
        "      most_prominent_topic = np.argmax(topic_scores[i])\n",
        "\n",
        "      # Get the location for this document\n",
        "      location = locations[i]\n",
        "\n",
        "      # Create the location_count dictionary if it doesn't exist for this topic\n",
        "      if most_prominent_topic not in location_count:\n",
        "          location_count[most_prominent_topic] = {}\n",
        "\n",
        "      # Update the count for this location and topic\n",
        "      if location in location_count[most_prominent_topic]:\n",
        "          location_count[most_prominent_topic][location] += 1\n",
        "      else:\n",
        "          location_count[most_prominent_topic][location] = 1\n",
        "\n",
        "  return location_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rakO67joZqdo"
      },
      "source": [
        "With this, we can use the `generate_location_data` function to aggregate the number of locations for each topic and then generate our Bar Graphs. Nothing more needs to be done here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLTWo6wjZaOr"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "state_count = generate_location_data(spacy_processed_docs, M, doc_locations, topic_model)\n",
        "\n",
        "# This code makes a bar graph displaying the popularity (as a proportion) per state...\n",
        "for k in range(best_k):\n",
        "  fig = plt.figure(figsize=(20,4))\n",
        "\n",
        "  threshold = 30\n",
        "\n",
        "  total_doc_counts = dict(Counter(doc_locations).items())\n",
        "  normalized_state_count = {loc : 100 * state_count[k][loc] / total_doc_counts[loc] for loc in state_count[k] if total_doc_counts[loc] > threshold}\n",
        "  state_items = sorted(normalized_state_count.items(), key = lambda item: item[1], reverse = True)\n",
        "  state_names = [item[0] for item in state_items]\n",
        "  count = np.array([item[1] for item in state_items])\n",
        "\n",
        "  plt.bar(state_names, count, color ='blue', width = .8)\n",
        "  plt.ylim([0, 100])\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.xlabel(\"States\")\n",
        "  plt.ylabel(\"Percentage of headlines from this location\")\n",
        "  plt.title(f\"Topic {k}: {', '.join(topics[k])}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk4O1XMdwdDP"
      },
      "source": [
        "Now, we'll use a library to plot the number of documents of each topic on a map, because it's much more visually intuitive than a bar graph!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_zToLg4hPC0"
      },
      "outputs": [],
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Our geolocation service\n",
        "geolocator = Nominatim(user_agent='myapplication')\n",
        "for k in range(best_k):\n",
        "  lon, lat = [], []\n",
        "\n",
        "  threshold = 30\n",
        "\n",
        "  total_doc_counts = dict(Counter(doc_locations).items())\n",
        "  normalized_state_count = {loc : 100 * state_count[k][loc] / total_doc_counts[loc] for loc in state_count[k] if total_doc_counts[loc] > threshold}\n",
        "  state_items = sorted(normalized_state_count.items(), key = lambda item: item[1], reverse = True)\n",
        "  state_names = [item[0] for item in state_items]\n",
        "  count = np.array([item[1] for item in state_items])\n",
        "\n",
        "  labels = []\n",
        "  # Assign each state a geographical location\n",
        "  for state, c in zip(state_names, count):\n",
        "    location = geolocator.geocode(state)\n",
        "    lat.append(location.latitude)\n",
        "    lon.append(location.longitude)\n",
        "    labels.append(f\"{state} : {c}% of all {state} headlines have this topic\")\n",
        "\n",
        "  marker = dict(color=\"blue\", size=count)\n",
        "  fig = go.Figure(data=go.Scattergeo(lon=lon, lat=lat, text=labels, mode='markers', marker=marker))\n",
        "  fig.update_layout(title = f\"Topic {k}: {', '.join(topics[k])}\" ,geo_scope='usa')\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwahJdZIuSdk"
      },
      "source": [
        "## 4. Conceptual Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZnl7hUauXoh"
      },
      "source": [
        "In this section, you may need to write more code to answer the conceptual questions—much of which will draw from code you've already written above. Do *not* change any code you've already written above; make duplicates in this section to edit, if necessary. Create markdown cells for written responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0McCQzd3ueUh"
      },
      "source": [
        "### Question 1: Threshold?\n",
        "When we plotted the bar graphs and geographical graphs, we included a line `threshold=30` to exclude some states.\n",
        "Create code cell(s) below to re-create the bar graphs with the same topic model without excluding any states. Which topics now have different top states? Using one of these topics and its new top state as an example, why was the threshold necessary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA7vHL-Sufj8"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "state_count = generate_location_data(spacy_processed_docs, M, doc_locations, topic_model)\n",
        "for k in range(best_k):\n",
        "  fig = plt.figure(figsize=(20,4))\n",
        "\n",
        "  threshold = ???\n",
        "  total_doc_counts = dict(Counter(doc_locations).items())\n",
        "  normalized_state_count = {loc : 100 * state_count[k][loc] / total_doc_counts[loc] for loc in state_count[k] if total_doc_counts[loc] > threshold}\n",
        "  state_items = sorted(normalized_state_count.items(), key = lambda item: item[1], reverse = True)\n",
        "  state_names = [item[0] for item in state_items]\n",
        "  count = np.array([item[1] for item in state_items])\n",
        "\n",
        "  plt.bar(state_names, count, color ='blue', width = .8)\n",
        "  plt.ylim([0, 100])\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.xlabel(\"States\")\n",
        "  plt.ylabel(\"Percentage of headlines from this location\")\n",
        "  plt.title(f\"Topic {k}: {', '.join(topics[k])}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWbJJioSuttg"
      },
      "source": [
        "ANSWER: [TODO]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwnXpXL2vUND"
      },
      "source": [
        "### Question 2: Perplexed on Perplexity\n",
        "We asked you to plot a perplexity graph to choose the number of topics that minimizes perplexity on held-out data. Please answer the following questions:\n",
        "- What is perplexity in the context of our topic model?\n",
        "- Why does perplexity not necessarily decrease by increasing the number of topics?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4rVMcaevWIY"
      },
      "source": [
        "ANSWER: [TODO]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YEINVNw5-W8"
      },
      "source": [
        "# Submission Instructions\n",
        "Please submit this Colab as **two** files to Gradescope:\n",
        "1.   A `.py` file: Click File > Download > Download .py\n",
        "2.   A `.ipynb` file ***WITH OUTPUTS***: Click Runtime > Run All, and then after all outputs are complete, click File > Download > Download .ipynb\n",
        "\n",
        "Please ensure that the `.ipynb` contains actual function outputs (and not leftover print statements, for instance). We will run your `.ipynb` file; if our outputs don't match the outputs in your submitted file (within reason), you will receive a 0 on this assignment.\n",
        "\n",
        "Please also keep any helper functions that you create for your implementation on the same cell and keep the cells to the function definition and print statements that we provided ONLY for best Autograder results. Thank you!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}