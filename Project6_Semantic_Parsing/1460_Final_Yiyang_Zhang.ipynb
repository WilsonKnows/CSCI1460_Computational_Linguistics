{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPK_c2EALwij"
      },
      "source": [
        "# Semantic Parsing Final Project\n",
        "Link to the paper: https://aclanthology.org/P16-1004.pdf\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a0Vn7FzeONE",
        "outputId": "83ba6574-4291-402d-993b-5272fff30d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install torch tqdm numpy datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0MLqDYLdLHF",
        "outputId": "d51e2b55-f9e8-44d4-d76d-9787b959739b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import random, torch, tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = \"drive/MyDrive/data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mewu8d2qACH"
      },
      "source": [
        "# Data Downloading\n",
        "This cell obtains the pre-processed Jobs dataset (see the paper) that you will be using to train and evaluate your model. (Pre-processed meaning that argument identification, section 3.6, has already been done for you). You should only need to run this cell ***once***. Feel free to delete it after running. Create a folder in your Google Drive in which the code below will store the pre-processed data needed for this project. Modify `FILEPATH` above to direct to said folder. It should start with `drive/MyDrive/...`, feel free to take a look at previous assignments that use mounting Google Drive if you can't remember what it should look like. *Make sure the data path ends with a slash character ('/').* The below code will access the zip file containing the pre-processed Jobs dataset from the paper and extract the files into your folder! Feel free to take a look at the `train.txt` and `test.txt` files to see what the data looks like. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXiL6mlFmssL",
        "outputId": "d265fa91-a357-4f03-c254-7e942dc6a359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "\n",
        "# https://stackoverflow.com/questions/31126596/saving-response-from-requests-to-file\n",
        "response = requests.get('http://dong.li/lang2logic/seq2seq_jobqueries.zip')\n",
        "if response.status_code == 200:\n",
        "  # https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
        "  with zipfile.ZipFile(io.BytesIO(response.content), \"r\") as zip_ref:\n",
        "    zip_ref.extractall(FILEPATH)\n",
        "  print(\"Extraction completed.\")\n",
        "else:\n",
        "  print(\"Failed to download the zip file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfJFfYRSFBV"
      },
      "source": [
        "# Data Pre-processing\n",
        "The following code is defined for you! It extracts the queries (inputs to your Seq2Seq model) and logical forms (expected outputs) from the training and testing files. It also does important pre-processing such as padding the queries and logical forms and turns the words into vocab indices. **Look over and understand this code before you start the assignment!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oEwaCwJhb9kL"
      },
      "outputs": [],
      "source": [
        "def extract_file(filename):\n",
        "  \"\"\"\n",
        "  Extracts queries and corresponding logical forms from either\n",
        "  train.txt or test.txt. (Feel free to take a look at the files themselves\n",
        "  in your Drive!)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename : str\n",
        "      name of the file to extract from\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[list[list[str]], list[list[str]]]\n",
        "      a tuple of a list of queries and their corresponding logical forms\n",
        "      each in the form of a list of string tokens\n",
        "  \"\"\"\n",
        "  queries, logical_forms = [], []\n",
        "  with open(FILEPATH + filename) as f:\n",
        "    for line in f:\n",
        "      line = line.strip() # remove new line character\n",
        "      query, logical_form = line.split('\\t')\n",
        "\n",
        "      query = query.split(' ')[::-1] # reversed inputs are used the paper (section 4.2)\n",
        "      logical_form = [\"<s>\"] + logical_form.split(' ') + [\"</s>\"]\n",
        "\n",
        "      queries.append(query)\n",
        "      logical_forms.append(logical_form)\n",
        "  return queries, logical_forms\n",
        "\n",
        "query_train, lf_train = extract_file('train.txt') # 500 instances\n",
        "query_test, lf_test = extract_file('test.txt') # 140 instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KEG4r-BpA3mH"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "query_vocab = Counter()\n",
        "for l in query_train:\n",
        "  query_vocab.update(l)\n",
        "\n",
        "query_word2idx = {}\n",
        "for w, c in query_vocab.items():\n",
        "  if c >= 2:\n",
        "    query_word2idx[w] = len(query_word2idx)\n",
        "query_word2idx['<UNK>'] = len(query_word2idx)\n",
        "query_word2idx['<PAD>'] = len(query_word2idx)\n",
        "query_idx2word = {i:word for word,i in query_word2idx.items()}\n",
        "\n",
        "query_vocab = list(query_word2idx.keys())\n",
        "\n",
        "lf_vocab = Counter()\n",
        "for lf in lf_train:\n",
        "  lf_vocab.update(lf)\n",
        "\n",
        "lf_vocab['<UNK>'] = 0\n",
        "lf_vocab['<PAD>'] = 0\n",
        "lf_idx2word = {i:word for i, word in enumerate(lf_vocab.keys())}\n",
        "lf_word2idx = {word:i for i, word in lf_idx2word.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6NH1EXAqDgnR"
      },
      "outputs": [],
      "source": [
        "query_train_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_train]\n",
        "query_test_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_test]\n",
        "\n",
        "lf_train_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_train]\n",
        "lf_test_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_test]\n",
        "\n",
        "def pad(seq, max_len, pad_token_idx):\n",
        "  \"\"\"\n",
        "  Pads a given sequence to the max length using the given padding token index\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  seq : list[int]\n",
        "      sequence in the form of a list of vocab indices\n",
        "  max_len : int\n",
        "      length sequence should be padded to\n",
        "  pad_token_idx\n",
        "      vocabulary index of the padding token\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  list[int]\n",
        "      padded sequence\n",
        "  \"\"\"\n",
        "  seq = seq[:max_len]\n",
        "  padded_seq = seq + (max_len - len(seq)) * [pad_token_idx]\n",
        "  return padded_seq\n",
        "\n",
        "query_max_target_len = max([len(i) for i in query_train_tokens])\n",
        "query_train_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_train_tokens]\n",
        "query_test_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_test_tokens]\n",
        "\n",
        "lf_max_target_len = int(max([len(i) for i in lf_train_tokens]) * 1.5)\n",
        "lf_train_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_train_tokens]\n",
        "lf_test_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_test_tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKjb4HsMKw-"
      },
      "source": [
        "# Data Loading\n",
        "The following code creates a JobsDataset and DataLoaders to use with your implemented model. Take a look at the main function at the end of this stencil to see how they are used in context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PginNNZ2sqqN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "\n",
        "# jobs_train, jobs_test = build_datasets()\n",
        "# dataloader_train, dataloader_test = build_dataloaders(jobs_train, jobs_test, train_batch_size=20)\n",
        "\n",
        "class JobsDataset(Dataset):\n",
        "  \"\"\"Defines a Dataset object for the Jobs dataset to be used with Dataloader\"\"\"\n",
        "  def __init__(self, queries, logical_forms):\n",
        "    \"\"\"\n",
        "    Initializes a JobsDataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    queries : list[list[int]]\n",
        "        a list of queries, which have been tokenized and padded, in the form\n",
        "        of a list of vocab indices\n",
        "    logical_forms : list[list[int]]\n",
        "        a list of corresponding logical forms, which have been tokenized and\n",
        "        padded, in the form of a list of vocab indices\n",
        "    \"\"\"\n",
        "    self.queries = queries\n",
        "    self.logical_forms = logical_forms\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the amount of paired queries and logical forms in the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    int\n",
        "        length of the dataset\n",
        "    \"\"\"\n",
        "    return len(self.queries)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple[list[int], list[int]]:\n",
        "    \"\"\"\n",
        "    Returns a paired query and logical form at the specified index\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    idx : int\n",
        "        specified index of the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[list[int], list[int]]\n",
        "        paired query and logical form at the specified index, in the form of\n",
        "        a list of vocab indices\n",
        "    \"\"\"\n",
        "    return self.queries[idx], self.logical_forms[idx]\n",
        "\n",
        "def build_datasets() -> tuple[JobsDataset, JobsDataset]:\n",
        "  \"\"\"\n",
        "  Builds a train and a test dataset from the queries and logical forms\n",
        "  train and test tokens\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[JobsDataset, JobsDataset]\n",
        "      a training and testing JobsDataset\n",
        "  \"\"\"\n",
        "  jobs_train = JobsDataset(queries=query_train_tokens, logical_forms=lf_train_tokens)\n",
        "  jobs_test = JobsDataset(queries=query_test_tokens, logical_forms=lf_test_tokens)\n",
        "  return jobs_train, jobs_test\n",
        "\n",
        "def collate(batch : list[tuple[list[int], list[int]]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch : list[tuple[list[int], list[int]]]\n",
        "      a list of outputs of __getitem__\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[torch.Tensor, torch.Tensor]\n",
        "      a batched set of input sequences and a batched set of target sequences\n",
        "  \"\"\"\n",
        "  src, tgt = default_collate(batch)\n",
        "  return torch.stack(src), torch.stack(tgt)\n",
        "\n",
        "def build_dataloaders(dataset_train: JobsDataset, dataset_test: JobsDataset,\n",
        "                      train_batch_size: int) -> tuple[DataLoader, DataLoader]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset, batching\n",
        "  the training data according to the inputted batch size and batching the\n",
        "  testing data with a batch size of 1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_train : JobsDataset\n",
        "      training dataset\n",
        "  dataset_test : JobsDataset\n",
        "      testing dataset\n",
        "  train_batch_size : int\n",
        "      batch size to be used during training\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[DataLoader, DataLoader]\n",
        "      a training and testing DataLoader\n",
        "  \"\"\"\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate)\n",
        "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate)\n",
        "  return dataloader_train, dataloader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDXsRIBIC42"
      },
      "source": [
        "# Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UbxqKYCKSVCm"
      },
      "outputs": [],
      "source": [
        "LF_SOS_INDEX = lf_word2idx['<s>']\n",
        "LF_EOS_INDEX = lf_word2idx['</s>']\n",
        "LF_PAD_INDEX = lf_word2idx['<PAD>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3vZdVRbvS8k9"
      },
      "outputs": [],
      "source": [
        "class Options:\n",
        "    rnn_size = 200\n",
        "    init_weight = 0.05\n",
        "    decay_rate = 0.95\n",
        "    learning_rate = 0.0025 # 0.005 for 5 epochs;\n",
        "    grad_clip = 5\n",
        "    dropout = 0\n",
        "    dropoutrec = 0\n",
        "    learning_rate_decay =  0.985\n",
        "    learning_rate_decay_after = 5\n",
        "    device = \"cuda\"\n",
        "\n",
        "opt = Options()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ue6o-D6lURSp"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"\n",
        "        Long Short-Term Memory (LSTM) cell implementation.\n",
        "\n",
        "        Parameters:\n",
        "        - rnn_size (int): The size of the hidden state and cell state.\n",
        "        - dropout (float): Dropout probability for regularization.\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.i2h = nn.Linear(opt.rnn_size, 4 * opt.rnn_size)\n",
        "        self.h2h = nn.Linear(opt.rnn_size, 4 * opt.rnn_size)\n",
        "        if opt.dropoutrec > 0:\n",
        "            self.dropout = nn.Dropout(opt.dropoutrec)\n",
        "\n",
        "    def forward(self, x, prev_c, prev_h):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the LSTM cell.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x (Tensor): The input tensor of shape (batch_size, input_size).\n",
        "        prev_c (Tensor): The previous cell state tensor of shape (batch_size, rnn_size).\n",
        "        prev_h (Tensor): The previous hidden state tensor of shape (batch_size, rnn_size).\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        cy (Tensor): The updated cell state tensor of shape (batch_size, rnn_size).\n",
        "        hy (Tensor): The updated hidden state tensor of shape (batch_size, rnn_size).\n",
        "\n",
        "        \"\"\"\n",
        "        gates = self.i2h(x) + self.h2h(prev_h)\n",
        "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
        "        ingate = torch.sigmoid(ingate)\n",
        "        forgetgate = torch.sigmoid(forgetgate)\n",
        "        cellgate = torch.tanh(cellgate)\n",
        "        outgate = torch.sigmoid(outgate)\n",
        "        if self.opt.dropoutrec > 0:\n",
        "            cellgate = self.dropout(cellgate)\n",
        "        cy = (forgetgate * prev_c) + (ingate * cellgate)\n",
        "        hy = outgate * torch.tanh(cy)  # n_b x hidden_dim\n",
        "        return cy, hy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9RREo_LUeMV"
      },
      "source": [
        "# RNN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bbKc7SVvUTnu"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, opt, input_size):\n",
        "        \"\"\"\n",
        "        Encoder recurrent neural network (RNN) module.\n",
        "\n",
        "        Parameters:\n",
        "        - input_size (int): The size of the input vocabulary.\n",
        "        - rnn_size (int): The size of the hidden state and cell state.\n",
        "        - dropout (float): Dropout probability for regularization.\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.hidden_size = opt.rnn_size\n",
        "        self.embedding = nn.Embedding(input_size, self.hidden_size)\n",
        "        # self.lstm = LSTM(self.opt)\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
        "        if opt.dropout > 0:\n",
        "            self.dropout = nn.Dropout(opt.dropout)\n",
        "\n",
        "    def forward(self, input_src, prev_c, prev_h):\n",
        "        \"\"\"\n",
        "        Forward pass through the Encoder RNN.\n",
        "\n",
        "        Parameters:\n",
        "        - input_src (Tensor): The input tensor of shape (batch_size, sequence_length).\n",
        "        - prev_c (Tensor): The previous cell state tensor of shape (1, batch_size, rnn_size).\n",
        "        - prev_h (Tensor): The previous hidden state tensor of shape (1, batch_size, rnn_size).\n",
        "\n",
        "        Returns:\n",
        "        - lstm_output (Tensor): The LSTM output tensor of shape (batch_size, sequence_length, rnn_size).\n",
        "        - (prev_cy, prev_hy) (tuple): Tuple containing updated cell state and hidden state tensors.\n",
        "          - prev_cy (Tensor): The updated cell state tensor of shape (batch_size, rnn_size).\n",
        "          - prev_hy (Tensor): The updated hidden state tensor of shape (batch_size, rnn_size).\n",
        "        \"\"\"\n",
        "        src_emb = self.embedding(input_src) # batch_size x src_length x emb_size\n",
        "        if self.opt.dropout > 0:\n",
        "            src_emb = self.dropout(src_emb)\n",
        "\n",
        "        lstm_output, (prev_cy, prev_hy) = self.lstm(src_emb, (prev_c, prev_h))\n",
        "        return lstm_output, (prev_cy.squeeze(0), prev_hy.squeeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4pfukGJVN1b"
      },
      "source": [
        "# Attension Union Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-p0dqOdcUVtC"
      },
      "outputs": [],
      "source": [
        "class AttnUnit(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Unit Module for sequence-to-sequence models.\n",
        "\n",
        "    This module calculates attention weights and produces a prediction based on the\n",
        "    encoder and decoder hidden states.\n",
        "\n",
        "    Parameters:\n",
        "    - opt (argparse.Namespace): Configuration options.\n",
        "    - output_size (int): Size of the output vocabulary.\n",
        "\n",
        "    Attributes:\n",
        "    - opt (argparse.Namespace): Configuration options.\n",
        "    - hidden_size (int): Size of the hidden state in the encoder and decoder.\n",
        "    - linear_att (nn.Linear): Linear layer for attention calculation.\n",
        "    - linear_out (nn.Linear): Linear layer for producing the final output.\n",
        "    - dropout (nn.Dropout, optional): Dropout layer for regularization.\n",
        "    - softmax (nn.Softmax): Softmax activation for attention weights.\n",
        "    - logsoftmax (nn.LogSoftmax): LogSoftmax activation for prediction.\n",
        "\n",
        "    Methods:\n",
        "    - forward(enc_s_top, dec_s_top): Forward pass through the attention unit.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, opt, output_size):\n",
        "        \"\"\"\n",
        "        Initializes the Attention Unit.\n",
        "\n",
        "        Parameters:\n",
        "        - opt: Configuration options.\n",
        "        - output_size (int): Size of the output vocabulary.\n",
        "        \"\"\"\n",
        "        super(AttnUnit, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.hidden_size = opt.rnn_size\n",
        "        # Linear layers for attention and output\n",
        "        self.linear_att = nn.Linear(2*self.hidden_size, self.hidden_size)\n",
        "        self.linear_out = nn.Linear(self.hidden_size, output_size)\n",
        "        # Dropout layer for regularization\n",
        "        if opt.dropout > 0:\n",
        "            self.dropout = nn.Dropout(opt.dropout)\n",
        "        # Activation functions\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, enc_s_top, dec_s_top):\n",
        "        \"\"\"\n",
        "        Forward pass through the attention unit.\n",
        "\n",
        "        Parameters:\n",
        "        - enc_s_top (torch.Tensor): Encoder hidden states with shape (batch_size, seq_length, hidden_size).\n",
        "        - dec_s_top (torch.Tensor): Decoder hidden states with shape (batch_size, hidden_size).\n",
        "\n",
        "        Returns:\n",
        "        - pred (torch.Tensor): Predicted log probabilities for the output sequence.\n",
        "        \"\"\"\n",
        "        # Calculate attention weights\n",
        "        dot = torch.bmm(enc_s_top, dec_s_top.unsqueeze(2))\n",
        "        attention = self.softmax(dot.squeeze(2)).unsqueeze(2)\n",
        "        # Apply attention to encoder hidden states\n",
        "        enc_attention = torch.bmm(enc_s_top.permute(0,2,1), attention)\n",
        "        # Concatenate attention output and decoder hidden state\n",
        "        hid = F.tanh(self.linear_att(torch.cat((enc_attention.squeeze(2),dec_s_top), 1)))\n",
        "        h2y_in = hid\n",
        "        # Apply dropout for regularization\n",
        "        if self.opt.dropout > 0:\n",
        "            h2y_in = self.dropout(h2y_in)\n",
        "        # Produce the final output\n",
        "        h2y = self.linear_out(h2y_in)\n",
        "        pred = self.logsoftmax(h2y)\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDEC-2jkzWj7"
      },
      "source": [
        "# Seq2SeqAttentionModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BHBjkuNnUzNE"
      },
      "outputs": [],
      "source": [
        "QUERY_VOCAB_LEN = len(query_vocab)\n",
        "LF_VOCAB_LEN = len(lf_vocab)\n",
        "\n",
        "# print(QUERY_VOCAB_LEN)\n",
        "# print(query_vocab)\n",
        "# print(LF_VOCAB_LEN)\n",
        "# print(lf_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vhV50b8TTUs5"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqAttentionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequence-to-Sequence Model with Attention.\n",
        "\n",
        "    This model comprises an encoder, a decoder, and an attention mechanism for\n",
        "    sequence-to-sequence tasks.\n",
        "\n",
        "    Parameters:\n",
        "    - opt: Configuration options.\n",
        "\n",
        "    Attributes:\n",
        "    - encoder (RNN): Encoder module for processing input sequences.\n",
        "    - decoder (RNN): Decoder module for generating output sequences.\n",
        "    - attention (AttnUnit): Attention unit for incorporating context information.\n",
        "    - optimizers (dict): Dictionary containing optimizers for encoder, decoder, and attention.\n",
        "    - criterion (torch.nn.CrossEntropyLoss): Loss criterion for training.\n",
        "\n",
        "    Methods:\n",
        "    - train(): Set the model to training mode.\n",
        "    - eval(): Set the model to evaluation mode.\n",
        "    - step(): Perform a gradient descent step for all optimizers.\n",
        "    - zero_grad(): Zero out the gradients for all optimizers.\n",
        "    - rate_decay(): Decay the learning rate for all optimizers.\n",
        "    - grad_clip(): Clip the gradients for all modules based on the specified threshold.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, opt):\n",
        "        super(Seq2SeqAttentionModel, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.encoder = RNN(self.opt, QUERY_VOCAB_LEN)\n",
        "        self.decoder = RNN(self.opt, LF_VOCAB_LEN)\n",
        "        self.attention = AttnUnit(self.opt, LF_VOCAB_LEN)\n",
        "        self.optimizers = {}\n",
        "        self.optimizers[\"encoder_optimizer\"] = optim.RMSprop(self.encoder.parameters(), lr=self.opt.learning_rate, alpha=self.opt.decay_rate)\n",
        "        self.optimizers[\"decoder_optimizer\"] = optim.RMSprop(self.decoder.parameters(), lr=self.opt.learning_rate, alpha=self.opt.decay_rate)\n",
        "        self.optimizers[\"attention_optimizer\"] = optim.RMSprop(self.attention.parameters(), lr=self.opt.learning_rate, alpha=self.opt.decay_rate)\n",
        "        self.criterion = nn.NLLLoss(size_average=False, ignore_index=lf_word2idx['<PAD>']) # torch.nn.CrossEntropyLoss(ignore_index=0) # size_average=False,\n",
        "        self.device = self.opt.device\n",
        "\n",
        "    # def forward(self, sentence: torch.LongTensor, form: torch.LongTensor, is_eval: bool=False):\n",
        "\n",
        "    #     cell_en = torch.zeros((sentence.size(1), opt.rnn_size), dtype=torch.float, requires_grad=True).to(sentence.device) # Cell state\n",
        "    #     hidden_en = torch.zeros((sentence.size(1), opt.rnn_size), dtype=torch.float, requires_grad=True).to(sentence.device) # Hidden state\n",
        "\n",
        "    #     # Initializae a tensor to store decoder's output\n",
        "    #     outputs = torch.zeros(form.size(0), form.size(1), LF_VOCAB_LEN).to(self.device) #???\n",
        "\n",
        "    #     # Last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
        "    #     enc_outputs, enc_states = self.encoder(sentence, cell_en.unsqueeze(0), hidden_en.unsqueeze(0))\n",
        "    #     dec_states = enc_states\n",
        "    #     decoder_input = torch.tensor([[LF_SOS_INDEX] * sentence.size(1)])\n",
        "\n",
        "    #     # Predict token by token\n",
        "    #     for i in range(form.size(0) - 1):\n",
        "    #         decoder_input = decoder_input.to(sentence.device)\n",
        "    #         dec_outputs, dec_states = self.decoder(decoder_input, dec_states[0].unsqueeze(0), dec_states[1].unsqueeze(0))\n",
        "    #         decoder_input = form[i + 1].unsqueeze(0) # Input current target for next interation\n",
        "    #         outputs[i] = self.attention(enc_outputs.transpose(0, 1), dec_states[0])\n",
        "\n",
        "    #         # if is_eval:\n",
        "    #         # # query = query_batch[i].unsqueeze(0) if teacher_forcing else best_pred.unsqueeze(0)\n",
        "    #         #   value, indice = outputs[i].topk(1)\n",
        "    #         #   decoder_input = indice.detach()\n",
        "\n",
        "    #         #   # Break if end-of-sequence token is predicted\n",
        "    #         #   if indice.item() == LF_EOS_INDEX:\n",
        "    #         #       break\n",
        "    #         # else:\n",
        "    #         #   decoder_input = form[i + 1].unsqueeze(0) # Input current target for next interation\n",
        "\n",
        "    #     return outputs\n",
        "\n",
        "    def train(self):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "        self.attention.train()\n",
        "\n",
        "    def eval(self):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()\n",
        "        self.attention.eval()\n",
        "\n",
        "    def step(self):\n",
        "        for optimizer in self.optimizers:\n",
        "            self.optimizers[optimizer].step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for optimizer in self.optimizers:\n",
        "            self.optimizers[optimizer].zero_grad()\n",
        "\n",
        "    def rate_decay(self):\n",
        "        for optimizer in self.optimizers:\n",
        "            for param_group in self.optimizers[optimizer].param_groups:\n",
        "                param_group['lr'] = param_group['lr'] * self.opt.learning_rate_decay\n",
        "\n",
        "    def grad_clip(self):\n",
        "        torch.nn.utils.clip_grad_value_(self.encoder.parameters(), self.opt.grad_clip)\n",
        "        torch.nn.utils.clip_grad_value_(self.decoder.parameters(), self.opt.grad_clip)\n",
        "        torch.nn.utils.clip_grad_value_(self.attention.parameters(), self.opt.grad_clip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NG376y1VUkOh"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "\n",
        "    model = Seq2SeqAttentionModel(opt=opt) # query_vocab_size, lf_vocab_size, hidden_size, output_size\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YiYNa1FINe6"
      },
      "source": [
        "# Training loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2OdOyg8RHrc1"
      },
      "outputs": [],
      "source": [
        "LF_SOS_INDEX = lf_word2idx['<s>']\n",
        "LF_EOS_INDEX = lf_word2idx['</s>']\n",
        "LF_PAD_INDEX = lf_word2idx['<PAD>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UT5eiZM0AnTf"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_dataloader: DataLoader, num_epochs: int=5,\n",
        "          device: str=\"cuda\") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Trains your model!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        your model!\n",
        "    train_dataloader : DataLoader\n",
        "        a dataloader of the training data from build_dataloaders\n",
        "    num_epochs : int\n",
        "        number of epochs to train for\n",
        "    device : str\n",
        "        device that the model is running on\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    nn.Module: Trained Seq2Seq model.\n",
        "\n",
        "    The training process involves iterating through the specified number of epochs, processing batches of training data,\n",
        "    and updating the model parameters based on the calculated loss. The training loss is printed after each epoch.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    print(\"Training...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      print(\"---Epoch {}---\\n\".format(epoch + 1))\n",
        "      model.train()\n",
        "\n",
        "      loss_sum = 0\n",
        "      for index, (sentence, form) in enumerate(train_dataloader):\n",
        "\n",
        "          # Zero the gradients to prepare for backpropagation\n",
        "          model.zero_grad()\n",
        "\n",
        "          sentence, form = sentence.to(device), form.to(device)\n",
        "\n",
        "          # Initialize cell and hidden states for the encoder\n",
        "          cell_en = torch.zeros((sentence.size(1), model.opt.rnn_size), dtype=torch.float, requires_grad=True).to(device)  # Cell state\n",
        "          hidden_en = torch.zeros((sentence.size(1), model.opt.rnn_size), dtype=torch.float, requires_grad=True).to(device)  # Hidden state\n",
        "\n",
        "          encoder_outputs, encoder_hidden = model.encoder(sentence, cell_en.unsqueeze(0), hidden_en.unsqueeze(0))\n",
        "\n",
        "          # Initialize the loss\n",
        "          loss = 0\n",
        "\n",
        "          # Initialize the decoder input with the start-of-sequence token\n",
        "          decoder_input = torch.tensor([[LF_SOS_INDEX] * sentence.size(1)], device=device)\n",
        "          decoder_hidden = encoder_hidden  # Use the last hidden state from the encoder to start the decoder\n",
        "\n",
        "          for i in range(form.size(0) - 1):  # Iterate over sequence\n",
        "              # Generating an output at each time step, and computing attention weights\n",
        "              # to focus on different parts of the input sequence during the decoding process\n",
        "              decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden[0].unsqueeze(0), decoder_hidden[1].unsqueeze(0))\n",
        "              decoder_input = form[i + 1].unsqueeze(0) # Input current target for next interation\n",
        "\n",
        "              # Calculate attention and accumulate the loss\n",
        "              pred = model.attention(encoder_outputs.transpose(0, 1), decoder_hidden[0])\n",
        "              loss += model.criterion(pred.squeeze(0), form[i + 1])\n",
        "          # logits = model.forward(sentence, form)\n",
        "          # logits, form = logits.to(device), form.to(device)\n",
        "          # loss = model.criterion(logits[1:].reshape(-1, logits.shape[-1]), form[1:].reshape(-1)) #!!!\n",
        "          # loss_sum += loss.item()\n",
        "\n",
        "          # Average the loss over the batch\n",
        "          loss = loss / sentence.size(1)\n",
        "          # Backpropagate the gradients\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip gradients if specified\n",
        "          if model.opt.grad_clip != -1:\n",
        "              model.grad_clip()\n",
        "\n",
        "          # Update the model parameters using the optimizer\n",
        "          model.step()\n",
        "\n",
        "      # loss_sum = loss.item() / len(train_dataloader)\n",
        "      loss_sum = loss / len(train_dataloader)\n",
        "\n",
        "      # Calculate and print the average loss per batch\n",
        "      print(\"Average Loss per Batch: {:.4f}\\n\".format(loss_sum))\n",
        "\n",
        "      # Decay the learning rate if specified\n",
        "      if model.opt.learning_rate_decay < 1:\n",
        "          if epoch >= model.opt.learning_rate_decay_after:\n",
        "              model.rate_decay()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z03LPaMPlpHO"
      },
      "source": [
        "# Evaluate loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nMrb0t96jwg5"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, dataloader: DataLoader, device: str=\"cuda\") -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Evaluates your model!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        your model!\n",
        "    dataloader : DataLoader\n",
        "        a dataloader of the testing data from build_dataloaders\n",
        "    device : str\n",
        "        device that the model is running on model\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[int, int]\n",
        "        per-token accuracy and exact_match accuracy\n",
        "\n",
        "    This function evaluates the model's performance on the given dataloader. It calculates\n",
        "    the per-token accuracy by comparing the predicted sequence with the target sequence,\n",
        "    excluding padding tokens. Additionally, it calculates the exact match accuracy by\n",
        "    checking if all predicted tokens match the target tokens for each sequence.\n",
        "\n",
        "    The model is expected to have an encoder-decoder architecture with an attention mechanism.\n",
        "    \"\"\"\n",
        "\n",
        "    # for epoch in range(epoch_num):\n",
        "    print(\"Predicting..\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    total_tokens = 0\n",
        "    total_correct = 0\n",
        "    exact_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for sentence, form in dataloader:\n",
        "\n",
        "            sentence, form = sentence.to(device), form.to(device)\n",
        "\n",
        "            # Initialize encoder hidden and cell states\n",
        "            hidden_en = torch.zeros((sentence.size(1), model.opt.rnn_size), dtype=torch.float, requires_grad=True).to(device)  # Hidden state\n",
        "            cell_en = torch.zeros((sentence.size(1), model.opt.rnn_size), dtype=torch.float, requires_grad=True).to(device)  # Cell state\n",
        "            # Encode the input sequence\n",
        "            encoder_outputs, encoder_states = model.encoder(sentence, hidden_en.unsqueeze(0), cell_en.unsqueeze(0))\n",
        "\n",
        "            # Initialize the first token in the decoder sequence (start-of-sequence)\n",
        "            prev = torch.tensor([[LF_SOS_INDEX]], device=device) # * sentence.size(1)\n",
        "            decoder_states = encoder_states\n",
        "\n",
        "            # Initialize lists to store predicted form and attention weights\n",
        "            predictions = []\n",
        "\n",
        "            # Decode the sequence\n",
        "            for index in range(form.size(0) - 1):\n",
        "              # Generating an output at each time step, and computing attention weights\n",
        "              # to focus on different parts of the input sequence during the decoding process\n",
        "              prev = prev.to(device)\n",
        "              decoder_output, decoder_states = model.decoder(prev, decoder_states[0].unsqueeze(0), decoder_states[1].unsqueeze(0))\n",
        "              pred = model.attention(encoder_outputs.transpose(0, 1), decoder_states[0])\n",
        "\n",
        "              # Choose the token with the highest probability\n",
        "              value = pred.argmax().item()\n",
        "              prev = torch.tensor([[value]])\n",
        "              predictions.append(value)\n",
        "\n",
        "              # Break if end-of-sequence token is predicted\n",
        "              if value == LF_EOS_INDEX:\n",
        "                  break\n",
        "\n",
        "            # Convert predictions list to a tensor\n",
        "            predictions = torch.tensor(predictions, device=device)\n",
        "\n",
        "            # Filter the PADDING terms\n",
        "            filtered_prediction = [tok for tok in predictions if tok != LF_PAD_INDEX]\n",
        "            filtered_form = [tok for tok in form[1:, :] if tok != LF_PAD_INDEX]\n",
        "\n",
        "            # Calculate per-token accuracy\n",
        "            token_accuracy = [p == t for p, t in zip(filtered_prediction, filtered_form)]\n",
        "            total_tokens += len(token_accuracy)\n",
        "\n",
        "            correct_tokens = sum(token_accuracy).item()\n",
        "            total_correct += correct_tokens\n",
        "\n",
        "            # Check if all tokens in a sequence are correct for exact match\n",
        "            exact_correct += int(all(token_accuracy))\n",
        "\n",
        "        per_token_accuracy = total_correct / total_tokens\n",
        "        exact_accuracy = exact_correct / len(dataloader.dataset)\n",
        "\n",
        "    return per_token_accuracy, exact_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOkicC3yLkfv"
      },
      "source": [
        "# Run this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qSnLCPeiI1N",
        "outputId": "e9e22788-50bd-4d2a-82b1-2f7e474ddc7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "---Epoch 1---\n",
            "\n",
            "Average Loss per Batch: 0.5247\n",
            "\n",
            "---Epoch 2---\n",
            "\n",
            "Average Loss per Batch: 0.2998\n",
            "\n",
            "---Epoch 3---\n",
            "\n",
            "Average Loss per Batch: 0.1871\n",
            "\n",
            "---Epoch 4---\n",
            "\n",
            "Average Loss per Batch: 0.1696\n",
            "\n",
            "---Epoch 5---\n",
            "\n",
            "Average Loss per Batch: 0.0993\n",
            "\n",
            "Predicting..\n",
            "Test Per-token Accuracy: 0.8954248366013072\n",
            "Test Exact-match Accuracy: 0.6285714285714286\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    jobs_train, jobs_test = build_datasets()\n",
        "    dataloader_train, dataloader_test = build_dataloaders(jobs_train, jobs_test, train_batch_size=20)\n",
        "    # print(dataloader_train) #!!!!!\n",
        "    model = create_model()\n",
        "    model = train(model, dataloader_train, num_epochs=5, device=\"cpu\") #device # Epochs = 5\n",
        "    test_per_token_accuracy, test_exact_match_accuracy = evaluate(model, dataloader_test, device=device)\n",
        "    print(f'Test Per-token Accuracy: {test_per_token_accuracy}')\n",
        "    print(f'Test Exact-match Accuracy: {test_exact_match_accuracy}')\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}